{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Training model to predict whether patient dies, or not.\n",
    "\n",
    "1) binary model (dead, not dead)\n",
    "2) multi-class classification model and using just the mRS6 classification outcome"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import importlib\n",
    "# Import local package\n",
    "from utils import waterfall\n",
    "# Force package to be reloaded\n",
    "importlib.reload(waterfall);\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the time duration to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    '''Singleton object for storing paths to data and database.'''\n",
    "\n",
    "  #  data_path: str = '../'\n",
    "  #  data_filename: str = 'SAMueL ssnap extract v2.csv'\n",
    "  #  data_save_path: str = './'\n",
    "  #  data_save_filename: str = 'reformatted_data.csv'\n",
    "  #  database_filename: str = 'samuel.db'\n",
    "  #  notebook: str = '01'\n",
    "  #  kfold_folder: str = 'data/kfold_5fold/'\n",
    "\n",
    "    data_read_path: str = '../data/'\n",
    "    data_read_filename: str = '02_reformatted_data_ml_230612.csv'\n",
    " #   data_save_path: str = './kfold_5fold'\n",
    "#    data_save_filename: str = 'train.csv'\n",
    "    notebook: str = '230725_'\n",
    "    model_text: str = 'xgb_all_data_binary'\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Data has previously been split into 5 stratified k-fold splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths.data_read_path + paths.data_read_filename\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to use onset to thrombolysis time in the model. Define function to calculate the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_onset_to_thrombolysis(row):\n",
    "    # Set default value of onset to thrombolysis of -100 (no thrombolysis given)\n",
    "    onset_to_thrombolysis = -100\n",
    "    # Set value if thrombolysis given\n",
    "    if  row['scan_to_thrombolysis_time'] != -100:\n",
    "        onset_to_thrombolysis = (row['onset_to_arrival_time'] + \n",
    "        row['arrival_to_scan_time'] + row['scan_to_thrombolysis_time'])\n",
    "    return onset_to_thrombolysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate onset to thgrombolysis (but set to -100 if no thrombolysis given)\n",
    "data['onset_to_thrombolysis_time'] = data.apply(calculate_onset_to_thrombolysis, axis=1)\n",
    "data.drop(['scan_to_thrombolysis_time', 'arrival_to_scan_time',\n",
    "        'onset_to_arrival_time'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 features\n"
     ]
    }
   ],
   "source": [
    "features = list(data)\n",
    "print(f\"There are {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the binary outcome: Dead or not dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"discharge_mrs6\"] = 0\n",
    "mask = data[\"discharge_disability\"] == 6\n",
    "data[\"discharge_mrs6\"][mask] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['prior_disability','stroke_severity','stroke_team',\n",
    "                     'onset_to_thrombolysis_time','age']\n",
    "X_data = data[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract two target features (one for binary model, one for multiclass model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_mc = data['discharge_disability']\n",
    "y_data_bin = data['discharge_mrs6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the balance of classes for the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of patients with death as six month outcome: 12.480952775634854%\n"
     ]
    }
   ],
   "source": [
    "proportion_die = y_data_bin.value_counts()[1]/y_data_bin.shape[0]\n",
    "print(f'Percent of patients with death as six month outcome: {proportion_die*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    137271\n",
       "1     19576\n",
       "Name: discharge_mrs6, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_bin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that classifies all as alive will have an accuracy of 87.5%. We hope that a model can improve on this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot the categorical feature (stroke team).\n",
    "\n",
    "Define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_to_one_hot(df, feature_name, prefix):\n",
    "    \"\"\"\n",
    "    df [dataframe]: training or test dataset\n",
    "    feature_name [str]: feature to convert to ont hot encoding\n",
    "    prefix [str]: string to use on new feature\n",
    "    \"\"\"\n",
    "\n",
    "    # One hot encode a feature\n",
    "    df_feature = pd.get_dummies(\n",
    "        df[feature_name], prefix = prefix)\n",
    "    df = pd.concat([df, df_feature], axis=1)\n",
    "    df.drop(feature_name, axis=1, inplace=True)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up two lists for the one hot encoding. \n",
    "\n",
    "A list of the feature names that are categorical and to be converted using one hot encoding.\n",
    "A list of the prefixes to use for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = convert_feature_to_one_hot(X_data, \"stroke_team\", \"team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of features in dataset, post one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ohe = list(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate some accuracy measures (returns a dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "\n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    false_negatives = (predicted_negatives == 1) & (observed_negatives == 0)\n",
    "       \n",
    "    sensitivity = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['accuracy'] = np.mean(predicted == observed)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide into training and test sets\n",
    "\n",
    "When we test a machine learning model we should always test it on data that has not been used to train the model. We will use sklearn's train_test_split method to randomly split the data: 75% for training, and 25% for testing.\n",
    "\n",
    "Use multiclass target feature for the split, then convert to the binary target feature (to ensure match of patient data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_mc, y_test_mc = train_test_split(X_data, y_data_mc, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_target(y_test_mc):\n",
    "    y_test_bin = 0\n",
    "    mask = y_test_mc == 6\n",
    "    y_test_bin[mask] = 1\n",
    "    return(y_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/pandas/core/series.py:1105\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_with_engine(key, value)\n\u001b[1;32m   1106\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[39m# We have a scalar (or for MultiIndex or object-dtype, scalar-like)\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m     \u001b[39m#  key that is not present in self.index.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/pandas/core/series.py:1175\u001b[0m, in \u001b[0;36mSeries._set_with_engine\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_with_engine\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   1177\u001b[0m     \u001b[39m# this is equivalent to self._values[key] = value\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/pandas/core/indexes/range.py:394\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[1;32m    395\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/pandas/core/indexes/base.py:5925\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5922\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5923\u001b[0m     \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5924\u001b[0m     \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5925\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: 152162     True\n146677    False\n5967       True\n125596    False\n139820    False\n          ...  \n107365    False\n18053     False\n141419    False\n77584     False\n127488    False\nName: discharge_disability, Length: 39212, dtype: bool",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m y_test_bin \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(\u001b[39m0\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdischarge_mrs6\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m#index=np.arange(y_test_mc.shape[0])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m mask \u001b[39m=\u001b[39m y_test_mc \u001b[39m==\u001b[39m \u001b[39m6\u001b[39m\n\u001b[0;32m----> 3\u001b[0m y_test_bin[mask] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      4\u001b[0m y_test_bin\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/pandas/core/series.py:1143\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m   1139\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mkey of type tuple not found and not a MultiIndex\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1140\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 1143\u001b[0m     key \u001b[39m=\u001b[39m check_bool_indexer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, key)\n\u001b[1;32m   1144\u001b[0m     key \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m   1146\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1147\u001b[0m         is_list_like(value)\n\u001b[1;32m   1148\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(value) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[39m#  _where call below\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m         \u001b[39m# GH#44265\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/pandas/core/indexing.py:2552\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2550\u001b[0m indexer \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_indexer_for(index)\n\u001b[1;32m   2551\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39min\u001b[39;00m indexer:\n\u001b[0;32m-> 2552\u001b[0m     \u001b[39mraise\u001b[39;00m IndexingError(\n\u001b[1;32m   2553\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnalignable boolean Series provided as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mindexer (index of the boolean Series and of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe indexed object do not match).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2556\u001b[0m     )\n\u001b[1;32m   2558\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   2560\u001b[0m \u001b[39m# fall through for boolean\u001b[39;00m\n",
      "\u001b[0;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "y_test_bin = pd.Series(0, name='discharge_mrs6')#index=np.arange(y_test_mc.shape[0])\n",
    "mask = y_test_mc == 6\n",
    "y_test_bin[mask] = 1\n",
    "y_test_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_train_bin \u001b[39m=\u001b[39m create_binary_target(y_train_mc)\n\u001b[1;32m      2\u001b[0m y_test_bin \u001b[39m=\u001b[39m create_binary_target(y_test_mc)\n",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m, in \u001b[0;36mcreate_binary_target\u001b[0;34m(y_test_mc)\u001b[0m\n\u001b[1;32m      2\u001b[0m y_test_bin \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      3\u001b[0m mask \u001b[39m=\u001b[39m y_test_mc \u001b[39m==\u001b[39m \u001b[39m6\u001b[39m\n\u001b[0;32m----> 4\u001b[0m y_test_bin[mask] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mreturn\u001b[39;00m(y_test_bin)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "y_train_bin = create_binary_target(y_train_mc)\n",
    "y_test_bin = create_binary_target(y_test_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Predict binary death (use defaulst threshold, 0.5)\n",
    "\n",
    "Fit XGBoost model, train model with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgboost(X_data, y_data, filename):\n",
    "    \n",
    "    # Check if exists\n",
    "    file_exists = exists(filename)\n",
    "\n",
    "    if file_exists:\n",
    "        # load model\n",
    "        with open(filename, 'rb') as filehandler:\n",
    "            model = pickle.load(filehandler)\n",
    "    else:        \n",
    "\n",
    "        # Define model\n",
    "        model = XGBClassifier(verbosity = 0, seed=42, learning_rate=0.5)\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(X_data, y_data)\n",
    "\n",
    "        # Save model\n",
    "        with open(filename, 'wb') as filehandler:\n",
    "            pickle.dump(model, filehandler)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = fit_xgboost(X_train, y_train_bin, f\"{paths.notebook}{paths.model_text}_bin.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_predicted_probabilities(model, X_data):\n",
    "    y_probs = model.predict_proba(X_data)\n",
    "    y_pred = model.predict(X_data)\n",
    "    return(y_probs, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_probs_bin, y_pred_bin) = calculate_predicted_probabilities(model_bin, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show accuracy (identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(y_test_bin, y_pred_bin)\n",
    "print (f'Accuracy (binary model): {accuracy[\"accuracy\"]:0.2f}')\n",
    "print (f'Sensitivity (binary model): {accuracy[\"sensitivity\"]:0.2f}')\n",
    "print (f'Specificity (binary model): {accuracy[\"specificity\"]:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_data, y_pred, xlabel, ylabel, title):\n",
    "    # Show confusion matrix\n",
    "    cm = confusion_matrix(y_data, y_pred)\n",
    "\n",
    "    # Visualize the confusion matrix with colors\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "    heatmap = plt.imshow(cm, cmap=plt.cm.Blues, origin='lower')\n",
    "    colorbar = plt.colorbar(heatmap, shrink=0.8, ax=ax1, alpha=0.5, label='Count')\n",
    "\n",
    "    # To add values to plot\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[0])):\n",
    "            plt.text(j, i, cm[i][j], ha='center', va='center', color='r')\n",
    "\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_ylabel(ylabel)\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test_bin, y_pred_bin, 'Predicted value', 'Observed value', 'Binary model (default threshold, 0.5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operator Characteristic and Sensitivity-Specificity Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic Curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(y_data, y_probs):\n",
    "    fpr, tpr, thresholds = roc_curve(y_data, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return(fpr, tpr, thresholds, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fpr_bin, tpr_bin, thresholds_bin, roc_auc_bin) = calculate_auc(y_test_bin, y_probs_bin[:,1])\n",
    "\n",
    "# Show area under curve  \n",
    "print (f'\\nAUC: {roc_auc_bin:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate data for sensitivity-specificity curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sensitivity_specificity(y_data, y_probs):\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "\n",
    "    # Loop through increments in probability of survival\n",
    "    thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "    for cutoff in thresholds: #  loop 0 --> 1 on steps of 0.1\n",
    "        # Get classificiation using cutoff\n",
    "        predicted_class = y_probs >= cutoff\n",
    "        predicted_class = predicted_class * 1.0\n",
    "        # Call accuracy measures function\n",
    "        accuracy = calculate_accuracy(y_data, predicted_class)\n",
    "        # Add accuracy scores to lists\n",
    "        sensitivity.append(accuracy['sensitivity'])\n",
    "        specificity.append(accuracy['specificity'])\n",
    "\n",
    "    return(sensitivity, specificity, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sensitivity_bin, specificity_bin, thresholds_bin) = calculate_sensitivity_specificity(y_test_bin, y_probs_bin[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a combined plot: ROC and sensitivity-specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_and_sen_spec(fpr, tpr, roc_auc, sensitivity, specificity, filename):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    # Plot ROC\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.plot(fpr, tpr, color='orange')\n",
    "    ax1.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('Receiver Operator Characteristic Curve')\n",
    "    text = f'AUC: {roc_auc:.3f}'\n",
    "    ax1.text(0.64,0.07, text, \n",
    "            bbox=dict(facecolor='white', edgecolor='black'))\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot sensitivity-specificity\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(sensitivity, specificity)\n",
    "    ax2.set_xlabel('Sensitivity')\n",
    "    ax2.set_ylabel('Specificity')\n",
    "    ax2.set_title('Sensitivity-Specificity Curve')\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_bin = f\"{paths.notebook}{paths.model_text}_bin_roc_sens_spec.jpg\"\n",
    "plot_roc_and_sen_spec(fpr_bin, tpr_bin, roc_auc_bin, sensitivity_bin, specificity_bin, filename_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify cross-over point on sensitivity-specificity curve\n",
    "Adjusting the classification threshold allows us to balance sensitivity (the proportion of patients receiving thrombolysis correctly identified) and specificity (the proportion of patients not receiving thrombolysis correctly identified). An increase in sensitivity causes a loss in specificity (and vice versa). Here we identify the pint where specificity and sensitivity hold the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect(a1, a2, b1, b2):\n",
    "    \"\"\" \n",
    "    Returns the point of intersection of the lines passing through a2,a1 and \n",
    "    b2,b1.\n",
    "    \n",
    "    a1: [x, y] a point on the first line\n",
    "    a2: [x, y] another point on the first line\n",
    "    b1: [x, y] a point on the second line\n",
    "    b2: [x, y] another point on the second line\n",
    "    \"\"\"\n",
    "    s = np.vstack([a1,a2,b1,b2])        # s for stacked\n",
    "    h = np.hstack((s, np.ones((4, 1)))) # h for homogeneous\n",
    "    l1 = np.cross(h[0], h[1])           # get first line\n",
    "    l2 = np.cross(h[2], h[3])           # get second line\n",
    "    x, y, z = np.cross(l1, l2)          # point of intersection\n",
    "    if z == 0:                          # lines are parallel\n",
    "        return (float('inf'), float('inf'))\n",
    "    return (x/z, y/z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intersect_parameters(sensitivity, specificity):\n",
    "    df = pd.DataFrame()\n",
    "    df['sensitivity'] = np.array(sensitivity)\n",
    "    df['specificity'] = np.array(specificity)\n",
    "    df['spec greater sens'] = df['specificity'] > df['sensitivity']\n",
    "\n",
    "    # find last index for senitivity being greater than specificity \n",
    "    mask = df['spec greater sens'] == False\n",
    "    last_id_sens_greater_spec = np.max(df[mask].index)\n",
    "    locs = [last_id_sens_greater_spec, last_id_sens_greater_spec + 1]\n",
    "    points = df.iloc[locs][['sensitivity', 'specificity']]\n",
    "\n",
    "    # Get intersetction with line of x=y\n",
    "    a1 = list(points.iloc[0].values)\n",
    "    a2 = list(points.iloc[1].values)\n",
    "    b1 = [0, 0]\n",
    "    b2 = [1, 1]\n",
    "\n",
    "    return(a1, a2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a1, a2, b1, b2) = create_intersect_parameters(sensitivity_bin, specificity_bin)\n",
    "\n",
    "intersection_bin = get_intersect(a1, a2, b1, b2)[0]\n",
    "\n",
    "print (f'\\nIntersection: {intersection_bin:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reliability_results(observed, probability):\n",
    "    # Get observed class and predicted probability\n",
    "#    observed = y_data_bin\n",
    "#    probability = y_probs_bin\n",
    "\n",
    "    # Bin data with numpy digitize (this will assign a bin to each case)\n",
    "    step = 0.10\n",
    "    bins = np.arange(step, 1+step, step)\n",
    "    digitized = np.digitize(probability, bins)\n",
    "        \n",
    "    # Put results in DataFrame\n",
    "    reliability = pd.DataFrame()\n",
    "    reliability['bin'] = digitized\n",
    "    reliability['probability'] = probability\n",
    "    reliability['observed'] = observed\n",
    "    classification = 1 * (probability > 0.5 )\n",
    "    reliability['correct'] = observed == classification\n",
    "    reliability['count'] = 1\n",
    "\n",
    "    # Summarise data by bin in new dataframe\n",
    "    reliability_summary = pd.DataFrame()\n",
    "\n",
    "    # Add bins and k-fold to summary\n",
    "    reliability_summary['bin'] = bins\n",
    "\n",
    "    # Calculate mean of predicted probability of thrombolysis in each bin\n",
    "    reliability_summary['confidence'] = \\\n",
    "        reliability.groupby('bin').mean()['probability']\n",
    "\n",
    "    # Calculate the proportion of patients who receive thrombolysis\n",
    "    reliability_summary['fraction_positive'] = \\\n",
    "        reliability.groupby('bin').mean()['observed']\n",
    "\n",
    "    # Calculate proportion correct in each bin\n",
    "    reliability_summary['fraction_correct'] = \\\n",
    "        reliability.groupby('bin').mean()['correct']\n",
    "\n",
    "    # Calculate fraction of results in each bin\n",
    "    reliability_summary['fraction_results'] = \\\n",
    "        reliability.groupby('bin').sum()['count'] / reliability.shape[0]\n",
    "    \n",
    "    return(reliability_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_callibration(reliability_summary):\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    # Plot predicted prob vs fraction psotive\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "    # Loop through k-fold reliability results\n",
    "    x = reliability_summary['confidence']\n",
    "    y = reliability_summary['fraction_positive']\n",
    "    ax1.plot(x,y, color='orange')\n",
    "    # Add 1:1 line\n",
    "    ax1.plot([0,1],[0,1], color='k', linestyle ='--')\n",
    "    # Refine plot\n",
    "    ax1.set_xlabel('Model probability')\n",
    "    ax1.set_ylabel('Fraction positive')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "\n",
    "    ax1.grid()\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "    #plt.savefig(f'./output/{notebook}_{model_text}_reliability.jpg', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_summary = calculate_reliability_results(y_test_bin, y_probs_bin[:,1])\n",
    "plot_callibration(reliability_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Predict binary death (use threshold for sensitivity v. specificity intersection)\n",
    "\n",
    "#### Fit XGBoost model, train model with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the threshold that gave this intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min(enumerate(sensitivity), key=lambda x: abs(x[1]-intersection))\n",
    "threshold_intersection_bin = min(range(len(sensitivity_bin)), \n",
    "                             key=lambda i: abs(sensitivity_bin[i]-intersection_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix with this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_intersection_bin = y_probs_bin[:,1] > thresholds_bin[threshold_intersection_bin]\n",
    "\n",
    "accuracy = calculate_accuracy(y_test_bin, y_pred_intersection_bin)\n",
    "\n",
    "print (f'Accuracy (binary model) [intersect threshold {thresholds_bin[threshold_intersection_bin]:0.4f}]: {accuracy[\"accuracy\"]:0.2f}')\n",
    "print (f'Sensitivity (binary model) [intersect threshold {thresholds_bin[threshold_intersection_bin]:0.4f}]: {accuracy[\"sensitivity\"]:0.2f}')\n",
    "print (f'Specificity (binary model) [intersect threshold {thresholds_bin[threshold_intersection_bin]:0.4f}]: {accuracy[\"specificity\"]:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test_bin, y_pred_intersection_bin, 'Predicted value', 'Observed value', f'Binary model (threshold determined from intersection: {threshold_intersection_bin:0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Predict multiclass death (use default threshold, 0.5)\n",
    "\n",
    "#### Fit XGBoost model, train model with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc = fit_xgboost(X_train, y_train_mc, f\"{paths.notebook}{paths.model_text}_mc.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_probs_mc, y_pred_mc) = calculate_predicted_probabilities(model_mc, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get just the probability for death (outcome 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mc6 = y_probs_mc[:,6] > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_mc = calculate_accuracy(y_test_bin, y_pred_mc6)\n",
    "print (f'Accuracy ((multiclass) [default threshold]: {accuracy_mc[\"accuracy\"]:0.2f}')\n",
    "print (f'Sensitivity (multiclass) [default threshold]: {accuracy_mc[\"sensitivity\"]:0.2f}')\n",
    "print (f'Specificity (multiclass) [default threshold]: {accuracy_mc[\"specificity\"]:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test_bin, y_pred_mc6, 'Predicted value', 'Observed value', 'Multiclass model (default threshold, 0.5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two different models (binary vs multiclass) predict the same outcome for each individual patient (when use default threshold, 0.5)?\n",
    "\n",
    "Plot a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_pred_bin, y_pred_mc6, \n",
    "                      'Predicted value from multiclass model', \n",
    "                      'Predicted value from binary model', \n",
    "                      'Comparing two models using default threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fpr_mc, tpr_mc, thresholds_mc, roc_auc_mc) = calculate_auc(y_test_bin, y_probs_mc[:,6])\n",
    "\n",
    "# Show area under curve  \n",
    "print (f'\\nAUC: {roc_auc_bin:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sensitivity_mc, specificity_mc, thresholds_mc) = calculate_sensitivity_specificity(y_test_bin, y_probs_mc[:,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_mc = f\"{paths.notebook}{paths.model_text}_mc_roc_sens_spec.jpg\"\n",
    "plot_roc_and_sen_spec(fpr_mc, tpr_mc, roc_auc_mc, sensitivity_mc, specificity_mc, filename_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a1, a2, b1, b2) = create_intersect_parameters(sensitivity_mc, specificity_mc)\n",
    "\n",
    "intersection_mc6 = get_intersect(a1, a2, b1, b2)[0]\n",
    "\n",
    "print (f'\\nIntersection: {intersection_mc6:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_summary_mc6 = calculate_reliability_results(y_test_bin, y_probs_mc[:,6])\n",
    "plot_callibration(reliability_summary_mc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Predict multiclass death (use threshold for sensitivity v. specificity intersection)\n",
    "\n",
    "#### Fit XGBoost model, train model with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find threshold to give this intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_intersection_mc6 = min(range(len(sensitivity_mc)), \n",
    "                             key=lambda i: abs(sensitivity_mc[i]-intersection_mc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_intersection_mc6 = y_probs_mc[:,6] > thresholds_mc[threshold_intersection_mc6]\n",
    "\n",
    "accuracy = calculate_accuracy(y_data_bin, y_pred_intersection_mc6)\n",
    "print (f'Accuracy (binary model) [intersect threshold {thresholds_mc[threshold_intersection_mc6]:0.4f}]: {accuracy[\"accuracy\"]:0.2f}')\n",
    "print (f'Sensitivity (binary model) [intersect threshold {thresholds_mc[threshold_intersection_mc6]:0.4f}]: {accuracy[\"sensitivity\"]:0.2f}')\n",
    "print (f'Specificity (binary model) [intersect threshold {thresholds_mc[threshold_intersection_mc6]:0.4f}]: {accuracy[\"specificity\"]:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix with this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test_bin, y_pred_intersection_mc6, \n",
    "                      'Predicted value', \n",
    "                      'Observed value', \n",
    "                      'Multiclass model using intersection threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two different models (binary vs multiclass) predict the same outcome for each individual patient (when use the threshold to give intersect for sensitivity and specificity)?\n",
    "\n",
    "Plot a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_pred_intersection_bin, y_pred_intersection_mc6, \n",
    "                      'Predicted value from multiclass model',\n",
    "                      'Predicted value from binary model',\n",
    "                      'Comparing two models using intersection threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('sam10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f85b883bff9a8a9f39576b94acbdf6672b3dc17c35647e7395f81e785740a4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
