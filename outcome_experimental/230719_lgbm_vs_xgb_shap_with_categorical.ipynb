{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost and LightGBM: using categorical variables (not one-hot encoding)\n",
    "\n",
    "Take small dataset, set stroke team feature as categorical (rather than one-hot encoded).\n",
    "\n",
    "Fit XGBoost and LightGBM and GradientBoostingClassifier multiclass classification model. \n",
    "\n",
    "Aim:\n",
    "* Explore how to calculate SHAP values.\n",
    "* What's the difference between providing background dataset vs not for TreeEnsemble?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages (3.3.5)\n",
      "Requirement already satisfied: scipy in /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages (from lightgbm) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages (from lightgbm) (1.2.2)\n",
      "Requirement already satisfied: wheel in /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages (from lightgbm) (0.38.4)\n",
      "Requirement already satisfied: numpy in /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "# pip install lightgbm if necessary\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import DMatrix\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# add histograms to dependency plots\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import math\n",
    "\n",
    "import importlib\n",
    "# Import local package\n",
    "from utils import waterfall\n",
    "# Force package to be reloaded\n",
    "importlib.reload(waterfall);\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the time duration to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    data_read_path: str = '../data/'\n",
    "    data_read_filename: str = '02_reformatted_data_ml_230709.csv'\n",
    "    notebook: str = '230719_'\n",
    "    model_text: str = 'lgbm_all_data_6_features'\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KP YOU ARE HERE.\n",
    "\n",
    "## Create toy dataset with known relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=500, centers=5, n_features=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths.data_read_path + paths.data_read_filename\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = data['discharge_disability'].unique()\n",
    "class_names = np.sort(class_names)\n",
    "n_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_data = data['discharge_disability'].head(5000)\n",
    "y_test_data = data['discharge_disability'].tail(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the X data, create two datasets (categorical and one hot encoded features).\n",
    "\n",
    "## Format data for categorical representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected_features = ['prior_disability','stroke_severity','stroke_team_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = {\n",
    "    'prior_disability': 'int',\n",
    "    'stroke_severity': 'int',\n",
    "    'stroke_team_id': 'category',\n",
    "    'discharge_disability': 'category'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_cat = data[X_selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set types\n",
    "for col in list(X_data_cat):\n",
    "    X_data_cat[col] = X_data_cat[col].astype(feature_types[col])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a reduced dataset to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data_cat = X_data_cat.head(5000)\n",
    "X_test_data_cat = X_data_cat.tail(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_data_cat = train_data_cat.drop('discharge_disability', axis=1)\n",
    "y_train_data_cat = train_data_cat['discharge_disability']\n",
    "\n",
    "X_test_data_cat = test_data_cat.drop('discharge_disability', axis=1)\n",
    "y_test_data_cat = test_data_cat['discharge_disability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cat = list(X_train_data_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data for one hot encoded feature representation\n",
    "\n",
    "Convert some categorical features to one hot encoded features.\n",
    "\n",
    "Define a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['prior_disability','stroke_severity','stroke_team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_ohe = data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_to_one_hot(df, feature_name, prefix):\n",
    "    \"\"\"\n",
    "    df [dataframe]: training or test dataset\n",
    "    feature_name [str]: feature to convert to ont hot encoding\n",
    "    prefix [str]: string to use on new feature\n",
    "    \"\"\"\n",
    "\n",
    "    # One hot encode a feature\n",
    "    df_feature = pd.get_dummies(\n",
    "        df[feature_name], prefix = prefix)\n",
    "    df = pd.concat([df, df_feature], axis=1)\n",
    "    df.drop(feature_name, axis=1, inplace=True)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_one_hot = [\"stroke_team\"]\n",
    "list_prefix = [\"team\"]\n",
    "\n",
    "for feature, prefix in zip(features_to_one_hot, list_prefix):\n",
    "    X_data_ohe = convert_feature_to_one_hot(X_data_ohe, feature, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a reduced dataset to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data_ohe = X_data_ohe.head(5000)\n",
    "X_test_data_ohe = X_data_ohe.tail(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_data_ohe = train_data_ohe.drop('discharge_disability', axis=1)\n",
    "y_train_data_ohe = train_data_ohe['discharge_disability']\n",
    "\n",
    "X_test_data_ohe = test_data_ohe.drop('discharge_disability', axis=1)\n",
    "y_test_data_ohe = test_data_ohe['discharge_disability']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of features in dataset, post one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ohe = list(X_train_data_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit four models\n",
    "\n",
    "LightGBM with stroke team as categorical\n",
    "LightGBM with stroke team as one hot encoded\n",
    "XGBoost with stroke team as categorical\n",
    "XGBoost with stroke team as one hot encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LGBM model\n",
    "\n",
    "Using categorical stroke team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "lgbm_model_cat = LGBMClassifier(random_state=42, learning_rate=0.3)\n",
    "\n",
    "# Fit model\n",
    "lgbm_model_cat.fit(X_train_data_cat, y_train_data)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = lgbm_model_cat.predict_proba(X_test_data_cat)\n",
    "y_pred = lgbm_model_cat.predict(X_test_data_cat)\n",
    "\n",
    "# Calculate error\n",
    "#(Need convert category type to int otherwise \"TypeError: Object with dtype category cannot perform the numpy op subtract\")\n",
    "y_error = y_test_data.astype(np.int8) - y_pred\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_error==0)\n",
    "print (f'Accuracy: {accuracy:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LGBM model\n",
    "\n",
    "Using one hot encoded stroke team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "lgbm_model_ohe = LGBMClassifier(random_state=42, learning_rate=0.3)\n",
    "\n",
    "# Fit model\n",
    "lgbm_model_ohe.fit(X_train_data_cat, y_train_data)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = lgbm_model_ohe.predict_proba(X_test_data_cat)\n",
    "y_pred = lgbm_model_ohe.predict(X_test_data_cat)\n",
    "\n",
    "# Calculate error\n",
    "y_error = y_test_data.astype(np.int8) - y_pred\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_error==0)\n",
    "print (f'Accuracy: {accuracy:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit XGBoost model\n",
    "\n",
    "Using categorical stroke team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "xgb_model_cat = XGBClassifier(seed=42, learning_rate=0.5,\n",
    "                          tree_method=\"gpu_hist\", enable_categorical=True)\n",
    "\n",
    "# Fit model\n",
    "xgb_model_cat.fit(X_train_data_cat, y_train_data)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = xgb_model_cat.predict_proba(X_test_data_cat)\n",
    "y_pred = xgb_model_cat.predict(X_test_data_cat)\n",
    "\n",
    "# Calculate error\n",
    "y_error = y_test_data.astype(np.int8) - y_pred\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_error==0)\n",
    "print (f'Accuracy: {accuracy:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit XGBoost model\n",
    "\n",
    "Using one hot encoding stroke team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "xgb_model_ohe = XGBClassifier(seed=42, learning_rate=0.5,\n",
    "                          tree_method=\"gpu_hist\")\n",
    "\n",
    "# Fit model\n",
    "xgb_model_ohe.fit(X_train_data_ohe, y_train_data)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = xgb_model_ohe.predict_proba(X_test_data_ohe)\n",
    "y_pred = xgb_model_ohe.predict(X_test_data_ohe)\n",
    "\n",
    "# Calculate error\n",
    "y_error = y_test_data - y_pred\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_error==0)\n",
    "print (f'Accuracy: {accuracy:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit GradientBoostingClassifier model\n",
    "\n",
    "Using categorical stroke team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "gbc_model_cat = GradientBoostingClassifier(random_state=42, learning_rate=0.5)#, enable_categorical=True)\n",
    "\n",
    "# Fit model\n",
    "gbc_model_cat.fit(X_train_data_cat, y_train_data)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = gbc_model_cat.predict_proba(X_test_data_cat)\n",
    "y_pred = gbc_model_cat.predict(X_test_data_cat)\n",
    "\n",
    "# Calculate error\n",
    "y_error = y_test_data.astype(np.int8) - y_pred\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_error==0)\n",
    "print (f'Accuracy: {accuracy:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit GradientBoostingClassifier model\n",
    "\n",
    "Using one hot encoded stroke team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "gbc_model_ohe = GradientBoostingClassifier(random_state=42, learning_rate=0.5)\n",
    "\n",
    "# Fit model\n",
    "gbc_model_ohe.fit(X_train_data_ohe, y_train_data)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = gbc_model_ohe.predict_proba(X_test_data_ohe)\n",
    "y_pred = gbc_model_ohe.predict(X_test_data_ohe)\n",
    "\n",
    "# Calculate error\n",
    "y_error = y_test_data - y_pred\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_error==0)\n",
    "print (f'Accuracy: {accuracy:0.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP values\n",
    "SHAP values give the contribution that each feature has on the models prediction, per instance. A SHAP value is returned for each feature, for each instance.\n",
    "\n",
    "We will use the shap library: https://shap.readthedocs.io/en/latest/index.html\n",
    "\n",
    "'Raw' SHAP values from XGBoost model are log odds ratios. A SHAP value is returned for each feature, for each instance, for each model (one per k-fold)\n",
    "\n",
    "## Get SHAP values\n",
    "TreeExplainer is a fast and exact method to estimate SHAP values for tree models and ensembles of trees. Using this we can calculate the SHAP values.\n",
    "\n",
    "Either load from pickle (if file exists), or calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for LightGBM model. Categorcial feature. With background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TreeEnsemble' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set up explainer using the model and feature values from training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer_lgb_cat_bg \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mTreeExplainer(lgbm_model_cat, X_train_data_cat)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m shap_values_extended_lgb_cat_bg \u001b[39m=\u001b[39m explainer_lgb_cat_bg(X_test_data_cat)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:181\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39melif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m    182\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         \u001b[39mraise\u001b[39;00m ExplainerError(\u001b[39m\"\u001b[39m\u001b[39mCurrently TreeExplainer can only handle models with categorical splits when \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m    184\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mfeature_perturbation=\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mtree_path_dependent\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m and no background data is passed. Please try again using \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m    185\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mshap.TreeExplainer(model, feature_perturbation=\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mtree_path_dependent\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:1107\u001b[0m, in \u001b[0;36mTreeEnsemble.predict\u001b[0;34m(self, X, y, output, tree_limit)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(X\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPassed input data matrix X must have 1 or 2 dimensions!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m \u001b[39mif\u001b[39;00m tree_limit \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m tree_limit \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m-> 1107\u001b[0m     tree_limit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m output \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogloss\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1110\u001b[0m     \u001b[39massert\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mBoth samples and labels must be provided when explaining the loss (i.e. `explainer.shap_values(X, y)`)!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TreeEnsemble' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_lgb_cat_bg = shap.TreeExplainer(lgbm_model_cat, X_train_data_cat)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_lgb_cat_bg = explainer_lgb_cat_bg(X_test_data_cat)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_lgb_cat_bg = shap_values_extended_lgb_cat_bg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for LightGBM model. Categorical feature. Without background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_lgb_cat = shap.TreeExplainer(lgbm_model_cat)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_lgb_cat = explainer_lgb_cat(X_test_data_cat)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_lgb_cat = shap_values_extended_lgb_cat.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for LightGBM model. One hot encoded feature. With background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TreeEnsemble' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set up explainer using the model and feature values from training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer_lgb_ohe_bg \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mTreeExplainer(lgbm_model_ohe, X_train_data_ohe)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m shap_values_extended_lgb_ohe_bg \u001b[39m=\u001b[39m explainer_lgb_ohe_bg(X_test_data_ohe)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:181\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39melif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m    182\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         \u001b[39mraise\u001b[39;00m ExplainerError(\u001b[39m\"\u001b[39m\u001b[39mCurrently TreeExplainer can only handle models with categorical splits when \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m    184\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mfeature_perturbation=\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mtree_path_dependent\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m and no background data is passed. Please try again using \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m    185\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mshap.TreeExplainer(model, feature_perturbation=\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mtree_path_dependent\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:1107\u001b[0m, in \u001b[0;36mTreeEnsemble.predict\u001b[0;34m(self, X, y, output, tree_limit)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(X\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPassed input data matrix X must have 1 or 2 dimensions!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m \u001b[39mif\u001b[39;00m tree_limit \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m tree_limit \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m-> 1107\u001b[0m     tree_limit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m output \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogloss\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1110\u001b[0m     \u001b[39massert\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mBoth samples and labels must be provided when explaining the loss (i.e. `explainer.shap_values(X, y)`)!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TreeEnsemble' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_lgb_ohe_bg = shap.TreeExplainer(lgbm_model_ohe, X_train_data_ohe)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_lgb_ohe_bg = explainer_lgb_ohe_bg(X_test_data_ohe)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_lgb_ohe_bg = shap_values_extended_lgb_ohe_bg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for LightGBM model. One hot encoded feature. Without background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] The number of features in data (120) is not the same as it was in training data (3).\n",
      "You can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "The number of features in data (120) is not the same as it was in training data (3).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m explainer_lgb_ohe \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mTreeExplainer(lgbm_model_ohe)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m shap_values_extended_lgb_ohe \u001b[39m=\u001b[39m explainer_lgb_ohe(X_test_data_ohe)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Shap values exist for each classification in a Tree\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m shap_values_lgb_ohe \u001b[39m=\u001b[39m shap_values_extended_lgb_ohe\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:217\u001b[0m, in \u001b[0;36mTree.__call__\u001b[0;34m(self, X, y, interactions, check_additivity)\u001b[0m\n\u001b[1;32m    214\u001b[0m     feature_names \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata_feature_names\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interactions:\n\u001b[0;32m--> 217\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshap_values(X, y\u001b[39m=\u001b[39;49my, from_call\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, check_additivity\u001b[39m=\u001b[39;49mcheck_additivity, approximate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapproximate)\n\u001b[1;32m    218\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(v) \u001b[39mis\u001b[39;00m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    219\u001b[0m         v \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(v, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# put outputs at the end\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:349\u001b[0m, in \u001b[0;36mTree.shap_values\u001b[0;34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlightgbm\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    348\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m approximate, \u001b[39m\"\u001b[39m\u001b[39mapproximate=True is not supported for LightGBM models!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 349\u001b[0m     phi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49moriginal_model\u001b[39m.\u001b[39;49mpredict(X, num_iteration\u001b[39m=\u001b[39;49mtree_limit, pred_contrib\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    350\u001b[0m     \u001b[39m# Note: the data must be joined on the last axis\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39moriginal_model\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/lightgbm/basic.py:3538\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   3536\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3537\u001b[0m         num_iteration \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 3538\u001b[0m \u001b[39mreturn\u001b[39;00m predictor\u001b[39m.\u001b[39;49mpredict(data, start_iteration, num_iteration,\n\u001b[1;32m   3539\u001b[0m                          raw_score, pred_leaf, pred_contrib,\n\u001b[1;32m   3540\u001b[0m                          data_has_header, is_reshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/lightgbm/basic.py:848\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    846\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pred_for_csc(data, start_iteration, num_iteration, predict_type)\n\u001b[1;32m    847\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 848\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__pred_for_np2d(data, start_iteration, num_iteration, predict_type)\n\u001b[1;32m    849\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    850\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/lightgbm/basic.py:938\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[39mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m    937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_predict(mat, start_iteration, num_iteration, predict_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/lightgbm/basic.py:908\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d.<locals>.inner_predict\u001b[0;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length of pre-allocated predict array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    907\u001b[0m out_num_preds \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int64(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 908\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterPredictForMat(\n\u001b[1;32m    909\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m    910\u001b[0m     ptr_data,\n\u001b[1;32m    911\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_ptr_data),\n\u001b[1;32m    912\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]),\n\u001b[1;32m    913\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m    914\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(C_API_IS_ROW_MAJOR),\n\u001b[1;32m    915\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(predict_type),\n\u001b[1;32m    916\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(start_iteration),\n\u001b[1;32m    917\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(num_iteration),\n\u001b[1;32m    918\u001b[0m     c_str(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpred_parameter),\n\u001b[1;32m    919\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(out_num_preds),\n\u001b[1;32m    920\u001b[0m     preds\u001b[39m.\u001b[39;49mctypes\u001b[39m.\u001b[39;49mdata_as(ctypes\u001b[39m.\u001b[39;49mPOINTER(ctypes\u001b[39m.\u001b[39;49mc_double))))\n\u001b[1;32m    921\u001b[0m \u001b[39mif\u001b[39;00m n_preds \u001b[39m!=\u001b[39m out_num_preds\u001b[39m.\u001b[39mvalue:\n\u001b[1;32m    922\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length for predict results\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(_LIB\u001b[39m.\u001b[39mLGBM_GetLastError()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: The number of features in data (120) is not the same as it was in training data (3).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_lgb_ohe = shap.TreeExplainer(lgbm_model_ohe)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_lgb_ohe = explainer_lgb_ohe(X_test_data_ohe)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_lgb_ohe = shap_values_extended_lgb_ohe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for XGBoost model. Categorical feature. With background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[09:35:11] ../src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\nStack trace:\n  [bt] (0) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x398643) [0x7f8103198643]\n  [bt] (1) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x399a02) [0x7f8103199a02]\n  [bt] (2) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2ca641) [0x7f81030ca641]\n  [bt] (3) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2e67a7) [0x7f81030e67a7]\n  [bt] (4) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGBoosterSaveModelToBuffer+0x2ad) [0x7f8102f3d76d]\n  [bt] (5) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(+0xa052) [0x7f8166e37052]\n  [bt] (6) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(+0x8925) [0x7f8166e35925]\n  [bt] (7) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(ffi_call+0xde) [0x7f8166e3606e]\n  [bt] (8) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x91e7) [0x7f8166e791e7]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set up explainer using the model and feature values from training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer_xgb_cat_bg \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mTreeExplainer(xgb_model_cat, X_train_data_cat)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m shap_values_extended_xgb_cat_bg \u001b[39m=\u001b[39m explainer_xgb_cat_bg(X_test_data_cat)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:149\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_perturbation \u001b[39m=\u001b[39m feature_perturbation\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m TreeEnsemble(model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_missing, model_output)\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m model_output\n\u001b[1;32m    151\u001b[0m \u001b[39m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:836\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxgboost\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    835\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_booster()\n\u001b[0;32m--> 836\u001b[0m xgb_loader \u001b[39m=\u001b[39m XGBTreeModelLoader(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moriginal_model)\n\u001b[1;32m    837\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees \u001b[39m=\u001b[39m xgb_loader\u001b[39m.\u001b[39mget_trees(data\u001b[39m=\u001b[39mdata, data_missing\u001b[39m=\u001b[39mdata_missing)\n\u001b[1;32m    838\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_offset \u001b[39m=\u001b[39m xgb_loader\u001b[39m.\u001b[39mbase_score\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:1431\u001b[0m, in \u001b[0;36mXGBTreeModelLoader.__init__\u001b[0;34m(self, xgb_model)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, xgb_model):\n\u001b[1;32m   1430\u001b[0m     \u001b[39m# new in XGBoost 1.1, 'binf' is appended to the buffer\u001b[39;00m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf \u001b[39m=\u001b[39m xgb_model\u001b[39m.\u001b[39;49msave_raw()\n\u001b[1;32m   1432\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinf\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1433\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf[\u001b[39m4\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/core.py:2411\u001b[0m, in \u001b[0;36mBooster.save_raw\u001b[0;34m(self, raw_format)\u001b[0m\n\u001b[1;32m   2409\u001b[0m cptr \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mPOINTER(ctypes\u001b[39m.\u001b[39mc_char)()\n\u001b[1;32m   2410\u001b[0m config \u001b[39m=\u001b[39m make_jcargs(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mraw_format)\n\u001b[0;32m-> 2411\u001b[0m _check_call(\n\u001b[1;32m   2412\u001b[0m     _LIB\u001b[39m.\u001b[39;49mXGBoosterSaveModelToBuffer(\n\u001b[1;32m   2413\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, config, ctypes\u001b[39m.\u001b[39;49mbyref(length), ctypes\u001b[39m.\u001b[39;49mbyref(cptr)\n\u001b[1;32m   2414\u001b[0m     )\n\u001b[1;32m   2415\u001b[0m )\n\u001b[1;32m   2416\u001b[0m \u001b[39mreturn\u001b[39;00m ctypes2buffer(cptr, length\u001b[39m.\u001b[39mvalue)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [09:35:11] ../src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\nStack trace:\n  [bt] (0) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x398643) [0x7f8103198643]\n  [bt] (1) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x399a02) [0x7f8103199a02]\n  [bt] (2) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2ca641) [0x7f81030ca641]\n  [bt] (3) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2e67a7) [0x7f81030e67a7]\n  [bt] (4) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGBoosterSaveModelToBuffer+0x2ad) [0x7f8102f3d76d]\n  [bt] (5) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(+0xa052) [0x7f8166e37052]\n  [bt] (6) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(+0x8925) [0x7f8166e35925]\n  [bt] (7) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(ffi_call+0xde) [0x7f8166e3606e]\n  [bt] (8) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x91e7) [0x7f8166e791e7]\n\n"
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_xgb_cat_bg = shap.TreeExplainer(xgb_model_cat, X_train_data_cat)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_xgb_cat_bg = explainer_xgb_cat_bg(X_test_data_cat)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_xgb_cat_bg = shap_values_extended_xgb_cat_bg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for XGBoost model. Categorical feature. Without background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[09:35:14] ../src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\nStack trace:\n  [bt] (0) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x398643) [0x7f8103198643]\n  [bt] (1) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x399a02) [0x7f8103199a02]\n  [bt] (2) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2ca641) [0x7f81030ca641]\n  [bt] (3) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2e67a7) [0x7f81030e67a7]\n  [bt] (4) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGBoosterSaveModelToBuffer+0x2ad) [0x7f8102f3d76d]\n  [bt] (5) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(+0xa052) [0x7f8166e37052]\n  [bt] (6) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(+0x8925) [0x7f8166e35925]\n  [bt] (7) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(ffi_call+0xde) [0x7f8166e3606e]\n  [bt] (8) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x91e7) [0x7f8166e791e7]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set up explainer using the model and feature values from training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer_xgb_cat \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mTreeExplainer(xgb_model_cat)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m shap_values_extended_xgb_cat \u001b[39m=\u001b[39m explainer_xgb_cat(X_test_data_cat)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:149\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_perturbation \u001b[39m=\u001b[39m feature_perturbation\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m TreeEnsemble(model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_missing, model_output)\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m model_output\n\u001b[1;32m    151\u001b[0m \u001b[39m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:836\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxgboost\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    835\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_booster()\n\u001b[0;32m--> 836\u001b[0m xgb_loader \u001b[39m=\u001b[39m XGBTreeModelLoader(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moriginal_model)\n\u001b[1;32m    837\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees \u001b[39m=\u001b[39m xgb_loader\u001b[39m.\u001b[39mget_trees(data\u001b[39m=\u001b[39mdata, data_missing\u001b[39m=\u001b[39mdata_missing)\n\u001b[1;32m    838\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_offset \u001b[39m=\u001b[39m xgb_loader\u001b[39m.\u001b[39mbase_score\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:1431\u001b[0m, in \u001b[0;36mXGBTreeModelLoader.__init__\u001b[0;34m(self, xgb_model)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, xgb_model):\n\u001b[1;32m   1430\u001b[0m     \u001b[39m# new in XGBoost 1.1, 'binf' is appended to the buffer\u001b[39;00m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf \u001b[39m=\u001b[39m xgb_model\u001b[39m.\u001b[39;49msave_raw()\n\u001b[1;32m   1432\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinf\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1433\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf[\u001b[39m4\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/core.py:2411\u001b[0m, in \u001b[0;36mBooster.save_raw\u001b[0;34m(self, raw_format)\u001b[0m\n\u001b[1;32m   2409\u001b[0m cptr \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mPOINTER(ctypes\u001b[39m.\u001b[39mc_char)()\n\u001b[1;32m   2410\u001b[0m config \u001b[39m=\u001b[39m make_jcargs(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mraw_format)\n\u001b[0;32m-> 2411\u001b[0m _check_call(\n\u001b[1;32m   2412\u001b[0m     _LIB\u001b[39m.\u001b[39;49mXGBoosterSaveModelToBuffer(\n\u001b[1;32m   2413\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, config, ctypes\u001b[39m.\u001b[39;49mbyref(length), ctypes\u001b[39m.\u001b[39;49mbyref(cptr)\n\u001b[1;32m   2414\u001b[0m     )\n\u001b[1;32m   2415\u001b[0m )\n\u001b[1;32m   2416\u001b[0m \u001b[39mreturn\u001b[39;00m ctypes2buffer(cptr, length\u001b[39m.\u001b[39mvalue)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [09:35:14] ../src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\nStack trace:\n  [bt] (0) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x398643) [0x7f8103198643]\n  [bt] (1) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x399a02) [0x7f8103199a02]\n  [bt] (2) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2ca641) [0x7f81030ca641]\n  [bt] (3) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2e67a7) [0x7f81030e67a7]\n  [bt] (4) /home/kerry/miniconda3/envs/sam10/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGBoosterSaveModelToBuffer+0x2ad) [0x7f8102f3d76d]\n  [bt] (5) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(+0xa052) [0x7f8166e37052]\n  [bt] (6) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(+0x8925) [0x7f8166e35925]\n  [bt] (7) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/../../libffi.so.8(ffi_call+0xde) [0x7f8166e3606e]\n  [bt] (8) /home/kerry/miniconda3/envs/sam10/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x91e7) [0x7f8166e791e7]\n\n"
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_xgb_cat = shap.TreeExplainer(xgb_model_cat)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_xgb_cat = explainer_xgb_cat(X_test_data_cat)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_xgb_cat = shap_values_extended_xgb_cat.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for XGBoost model. One hot encoded feature. With background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 6918/7000 [00:51<00:00]        "
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_xgb_ohe_bg = shap.TreeExplainer(xgb_model_ohe, X_train_data_ohe)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_xgb_ohe_bg = explainer_xgb_ohe_bg(X_test_data_ohe)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_xgb_ohe_bg = shap_values_extended_xgb_ohe_bg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for XGBoost model. One hot encoded feature. Without background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_xgb_ohe = shap.TreeExplainer(xgb_model_ohe)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_xgb_ohe = explainer_xgb_ohe(X_test_data_ohe)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_xgb_ohe = shap_values_extended_xgb_ohe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for GradientBoosterClassifier. Categorical feature. With background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "GradientBoostingClassifier is only supported for binary classification right now!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set up explainer using the model and feature values from training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer_gbc_cat_bg \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mTreeExplainer(gbc_model_cat, X_train_data_cat)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m shap_values_extended_gbc_cat_bg \u001b[39m=\u001b[39m explainer_gbc_cat_bg(X_test_data_cat)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:149\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_perturbation \u001b[39m=\u001b[39m feature_perturbation\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m TreeEnsemble(model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_missing, model_output)\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m model_output\n\u001b[1;32m    151\u001b[0m \u001b[39m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:778\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m# TODO: deal with estimators for each class\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 778\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mGradientBoostingClassifier is only supported for binary classification right now!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m \u001b[39m# currently we only support the logs odds estimator\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m safe_isinstance(model\u001b[39m.\u001b[39minit_, [\u001b[39m\"\u001b[39m\u001b[39msklearn.ensemble.LogOddsEstimator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msklearn.ensemble.gradient_boosting.LogOddsEstimator\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: GradientBoostingClassifier is only supported for binary classification right now!"
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_gbc_cat_bg = shap.TreeExplainer(gbc_model_cat, X_train_data_cat)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_gbc_cat_bg = explainer_gbc_cat_bg(X_test_data_cat)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_gbc_cat_bg = shap_values_extended_gbc_cat_bg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for GradientBoosterClassifier. Categorical feature. Without background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "GradientBoostingClassifier is only supported for binary classification right now!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set up explainer using the model and feature values from training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer_gbc_cat \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mTreeExplainer(gbc_model_cat)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m shap_values_extended_gbc_cat \u001b[39m=\u001b[39m explainer_gbc_cat(X_test_data_cat)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:149\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_perturbation \u001b[39m=\u001b[39m feature_perturbation\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m TreeEnsemble(model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_missing, model_output)\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m model_output\n\u001b[1;32m    151\u001b[0m \u001b[39m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:778\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m# TODO: deal with estimators for each class\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 778\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mGradientBoostingClassifier is only supported for binary classification right now!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m \u001b[39m# currently we only support the logs odds estimator\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m safe_isinstance(model\u001b[39m.\u001b[39minit_, [\u001b[39m\"\u001b[39m\u001b[39msklearn.ensemble.LogOddsEstimator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msklearn.ensemble.gradient_boosting.LogOddsEstimator\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: GradientBoostingClassifier is only supported for binary classification right now!"
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_gbc_cat = shap.TreeExplainer(gbc_model_cat)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_gbc_cat = explainer_gbc_cat(X_test_data_cat)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_gbc_cat = shap_values_extended_gbc_cat.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for GradientBoosterClassifier. One hot encoded feature. With background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "GradientBoostingClassifier is only supported for binary classification right now!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set up explainer using the model and feature values from training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer_gbc_ohe_bg \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mTreeExplainer(gbc_model_ohe, X_train_data_ohe)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m shap_values_extended_gbc_ohe_bg \u001b[39m=\u001b[39m explainer_gbc_ohe_bg(X_test_data_ohe)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:149\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_perturbation \u001b[39m=\u001b[39m feature_perturbation\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m TreeEnsemble(model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_missing, model_output)\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m model_output\n\u001b[1;32m    151\u001b[0m \u001b[39m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:778\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m# TODO: deal with estimators for each class\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 778\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mGradientBoostingClassifier is only supported for binary classification right now!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m \u001b[39m# currently we only support the logs odds estimator\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m safe_isinstance(model\u001b[39m.\u001b[39minit_, [\u001b[39m\"\u001b[39m\u001b[39msklearn.ensemble.LogOddsEstimator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msklearn.ensemble.gradient_boosting.LogOddsEstimator\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: GradientBoostingClassifier is only supported for binary classification right now!"
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_gbc_ohe_bg = shap.TreeExplainer(gbc_model_ohe, X_train_data_ohe)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_gbc_ohe_bg = explainer_gbc_ohe_bg(X_test_data_ohe)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_gbc_ohe_bg = shap_values_extended_gbc_ohe_bg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SHAP for GradientBoosterClassifier model. One hot encoded feature. Without background data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "GradientBoostingClassifier is only supported for binary classification right now!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set up explainer using the model and feature values from training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer_gbc_ohe \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mTreeExplainer(gbc_model_ohe)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Get (and store) Shapley values along with base and feature values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m shap_values_extended_gbc_ohe \u001b[39m=\u001b[39m explainer_gbc_ohe(X_test_data_ohe)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:149\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_perturbation \u001b[39m=\u001b[39m feature_perturbation\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m TreeEnsemble(model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_missing, model_output)\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m model_output\n\u001b[1;32m    151\u001b[0m \u001b[39m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/shap/explainers/_tree.py:778\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m# TODO: deal with estimators for each class\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 778\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mGradientBoostingClassifier is only supported for binary classification right now!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m \u001b[39m# currently we only support the logs odds estimator\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m safe_isinstance(model\u001b[39m.\u001b[39minit_, [\u001b[39m\"\u001b[39m\u001b[39msklearn.ensemble.LogOddsEstimator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msklearn.ensemble.gradient_boosting.LogOddsEstimator\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: GradientBoostingClassifier is only supported for binary classification right now!"
     ]
    }
   ],
   "source": [
    "# Set up explainer using the model and feature values from training set\n",
    "explainer_gbc_ohe = shap.TreeExplainer(gbc_model_ohe)\n",
    "\n",
    "# Get (and store) Shapley values along with base and feature values\n",
    "shap_values_extended_gbc_ohe = explainer_gbc_ohe(X_test_data_ohe)\n",
    "\n",
    "# Shap values exist for each classification in a Tree\n",
    "# We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "shap_values_gbc_ohe = shap_values_extended_gbc_ohe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, can calculate SHAP values for:\n",
    "\n",
    "1. LightGBM with categorical features, without background data (0.6 seconds)\n",
    "\n",
    "2. XGBoost with one hot encoded features, with background data (53 seconds)\n",
    "\n",
    "3. XGBoost with one hot encoded features, without background data (0.3 seconds)\n",
    "\n",
    "\n",
    "Can not calculate SHAP values for:\n",
    "\n",
    "4. LightGBM with categorical features, with background data\n",
    "--> AttributeError: 'TreeEnsemble' object has no attribute 'values'\n",
    "\n",
    "Error meaning - SHAP needs to access the value that the node is split on (when using background data), however when have categorical data types the node does not have a single value, instead it was groups of classes. SHAP does not yet work with this data format.\n",
    "\n",
    "5. LightGBM with one hot encoded features, without background data\n",
    "--> \"name\": \"LightGBMError\",\n",
    "\t\"message\": \"The number of features in data (120) is not the same as it was in training data (3).\\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\",\n",
    "\n",
    "6. LightGBM with one hot encoded features, with background data\n",
    "--> AttributeError: 'TreeEnsemble' object has no attribute 'values'\n",
    "\n",
    "Error meaning - I don't understand why this has an error here, as my understanding of the error was that there was not a value being used as a node split, but this on;y has values as is using one-hot encoded features.\n",
    "\n",
    "7. XGBoost with categorical features, with background data\n",
    "--> XGBoostError: [10:18:35] ../src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\n",
    "\n",
    "Error meaning - https://github.com/shap/shap/issues/2662\n",
    "\n",
    "When using Pandas categorical dtypes as inputs to XGBoost models (to avoid the need for encoding), XGBoost only allows you to serialise the model using JSON / UBJSON. SHAP currently loads the trees in a model by reading a raw binary dump generated via the save_raw() method, so it can't load models that make use of this functionality.\n",
    "\n",
    "A solution could be for SHAP to have the functionality to use the JSON dump instead to load tree data if enable_categorical is true for the model. It does not yet, nor does it look like support for JSON-serialized models will be added to SHAP anytime soon (guess that the developers chose not to support the JSON/UBJSON serialization of XGBoost models because JSON doesn't specify a precision for floating point values. In edge cases where this precision becomes important to determine the result of a node's split, such information may be lost when we save in the JSON format. If we wanted to force XGBTreeModelLoader to accept JSON-serialized models regardless, we will need to rewrite the entire __init__() because it has been built around the default binary dump returned by xgboost.Booster.save_raw(). Until XGBoost supports serialization of categorical-enabled models in formats other than JSON/UBJSON, we might not be able to use TreeExplainer with such models.)\n",
    "\n",
    "I don't think it's possible to get a quick fix.\n",
    "\n",
    "Questionable work around options (another got same error with xgb.train using tree_method=\"approx\" or \"hist\". Maybe these solutions gave no errors because you were using \"exact\" which doesn't use categorical features?):\n",
    "* Use xgb.train (the traditional method), apparently there is still no support for one of scikit-learn version (XGBClassifier).\n",
    "* NOT TO USE tree_method='gpu_hist' in your model (at least on the Explainer object). I was facing the same issue for several time and removed it from my model object. After that, everything works fine.\n",
    "\n",
    "Supported features for the four tree methods: Exact, Approx, Hist GPU Hist\n",
    "https://xgboost.readthedocs.io/en/stable/treemethod.html#feature-matrix\n",
    "\n",
    "Perhaps the most practical solution right now would be to use LightGBM's LGBMClassifier. LightGBM implements the same optimal partitioning technique to handle categorical data that is still experimental in XGBoost. I've tested LGBMClassifier and can confirm it works with TreeExplainer. It has a sklearn API too, so it might be able to fulfil your PMML needs. However the visualizations (e.g. beeswarm plot, force plots) are broken for categorical variables (even with lgbm).\n",
    "\n",
    "Work around:\n",
    "* Do not use SHAP library.\n",
    "https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.Booster.predict\n",
    "\n",
    "If we set the pred_contribs == True, the we get the shap values as output from the predict method.\n",
    "\tshap_values = self.model.predict(xgb.DMatrix(self.data, enable_categorical=True), pred_contribs=True)\n",
    "\n",
    "Remove the last column because it contains the bias term:\n",
    "\tshap_values[:, :-1]\n",
    "\n",
    "The equivalent of the explainer.expected_value when using the pred_contribs parameter is the last column of the shap_values array. It's a constant value, so you can just take the first row: \n",
    "\tshap_values[0, -1]\n",
    "\n",
    "Making the low-level XGBoost be a classification model: https://www.datacamp.com/tutorial/xgboost-in-python\n",
    "\n",
    "8. XGBoost with categorical features, without background data\n",
    "--> XGBoostError: [10:18:39] ../src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\n",
    "\n",
    "Error meaning - See above (model 7.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and make the models work using understanding from the error messages.\n",
    "\n",
    "Use .predict with pred_contribs=True for XGBoost when using categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.predict() got an unexpected keyword argument 'pred_contribs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m shap_values_xgb_cat \u001b[39m=\u001b[39m xgb_model_cat\u001b[39m.\u001b[39;49mpredict(DMatrix(X_test_data_cat, enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), pred_contribs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: XGBClassifier.predict() got an unexpected keyword argument 'pred_contribs'"
     ]
    }
   ],
   "source": [
    "shap_values_xgb_cat = xgb_model_cat.predict(DMatrix(X_test_data_cat, enable_categorical=True), pred_contribs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to use the low-level XGBoost to make use of pred_contribs=True argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to a native API model\n",
    "xgb_model_nativeapi_cat = xgb_model_cat.get_booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_xgb_nativeapi_cat = xgb_model_nativeapi_cat.predict(DMatrix(X_test_data_cat, enable_categorical=True), pred_contribs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values_xgb_nativeapi_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmatrix_train =  DMatrix(X_train_data_cat, label=y_train_data, enable_categorical=True)\n",
    "xgb_model_nativeapi_cat1 = xgb.train({\"objective\": \"multi:softprob\", \"learning_rate\": 0.5, \"seed\":42,\"num_class\": n_classes}, dmatrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmatrix_test =  DMatrix(X_test_data_cat, label=y_test_data, enable_categorical=True)\n",
    "shap_values_xgb_nativeapi_cat1 = xgb_model_nativeapi_cat1.predict(dmatrix_test, pred_contribs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7, 4)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values_xgb_nativeapi_cat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 6, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values_xgb_nativeapi_cat1[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's fitted a regression. Need to force it to fit a classificatino model\n",
    "https://www.datacamp.com/tutorial/xgboost-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Encode y to numeric\n",
    "y_encoded = OrdinalEncoder().fit_transform(data['discharge_disability'])\n",
    "\n",
    "y_encoded_train_data = y_encoded.head(5000)\n",
    "y_encoded_test_data = y_encoded.tail(1000)\n",
    "\n",
    "# Extract text features\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Convert to pd.Categorical\n",
    "for col in cats:\n",
    "   X[col] = X[col].astype('category')\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)\n",
    "\n",
    "# Create classification matrices\n",
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "\n",
    "params = {\"objective\": \"multi:softprob\", \"tree_method\": \"gpu_hist\", \"num_class\": n_classes, \"learning_rate\": 0.5, \"seed\":42}\n",
    "\n",
    "xgb_model_ll_cat = xgb.train(params, dtrain_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last column ontains the bias term (expected value) It's a constant value (so you can just take the first row)\n",
    "shap_values_xgb_cat[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_xgb_cat[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the variation in the SHAP values for these three different SHAP calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_xgb_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_xgb_ohe_bg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_lgb_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 6\n",
    "row = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_xgb_ohe, X_test_data_ohe.values, \n",
    "                  plot_type=\"bar\", class_names=class_names, \n",
    "                  feature_names=X_test_data_ohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_xgb_ohe_bg, X_test_data_ohe.values, \n",
    "                  plot_type=\"bar\", class_names=class_names, \n",
    "                  feature_names=X_test_data_ohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_lgb_cat, X_test_data_cat.values, \n",
    "                  plot_type=\"bar\", class_names=class_names, \n",
    "                  feature_names=X_test_data_cat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = shap.summary_plot(shap_values_xgb_ohe[cat], X_test_data_ohe.values, \n",
    "                       feature_names=X_test_data_ohe.columns, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = shap.summary_plot(shap_values_xgb_ohe_bg[cat], X_test_data_cat.values, \n",
    "                       feature_names=X_test_data_cat.columns, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = shap.summary_plot(shap_values_lgb_cat[cat], X_test_data_cat.values, \n",
    "                       feature_names=X_test_data_cat.columns, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.waterfall_plot(shap.Explanation(values=shap_values_xgb_ohe[cat][row], \n",
    "                                     base_values=explainer_xgb_ohe.expected_value[cat], \n",
    "                                     data=X_test_data_ohe.iloc[row],\n",
    "                                     feature_names=X_test_data_ohe.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.waterfall_plot(shap.Explanation(values=shap_values_xgb_ohe_bg[cat][row], \n",
    "                                     base_values=explainer_xgb_ohe.expected_value[cat], \n",
    "                                     data=X_test_data_ohe.iloc[row],\n",
    "                                     feature_names=X_test_data_ohe.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot waterfall plots for one patient for each of the seven classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.waterfall_plot(shap.Explanation(values=shap_values_lgb_cat[cat][row], \n",
    "                                     base_values=explainer_lgb_cat.expected_value[cat], \n",
    "                                     data=X_test_data_cat.iloc[row],\n",
    "                                     feature_names=X_test_data_cat.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of subplots per class. Each showing the relationship between\n",
    "# each combination of features on the SHAP value.\n",
    "# setup matrix of subplots\n",
    "fig = plt.figure()\n",
    "\n",
    "# Set overall title\n",
    "fig.suptitle(f'Waterfall for instance {row} '\n",
    "             f'(observed dischange mRS = {y_test_data_ohe.iloc[row]})', \n",
    "             fontsize=30)\n",
    "\n",
    "# Initialise subplot counter\n",
    "count = 1\n",
    "\n",
    "for c in range(n_classes):\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    shap.waterfall_plot(shap.Explanation(values=shap_values_xgb_ohe[c][row], \n",
    "                                        base_values=explainer_xgb_ohe.expected_value[c], \n",
    "                                        data=X_test_data_ohe.iloc[row],  \n",
    "                                        feature_names=X_test_data_ohe.columns.tolist()), \n",
    "                                        show=False)\n",
    "    ax. set_title(f\"Predicted discharge mRS {c}\")\n",
    "    # Increase subplot counter\n",
    "    count+=1\n",
    "    \n",
    "plt.gcf().set_size_inches(20,15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of subplots per class. Each showing the relationship between\n",
    "# each combination of features on the SHAP value.\n",
    "# setup matrix of subplots\n",
    "fig = plt.figure()\n",
    "\n",
    "# Set overall title\n",
    "fig.suptitle(f'Waterfall for instance {row} '\n",
    "             f'(observed dischange mRS = {y_test_data_ohe.iloc[row]})', \n",
    "             fontsize=30)\n",
    "\n",
    "# Initialise subplot counter\n",
    "count = 1\n",
    "\n",
    "for c in range(n_classes):\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    shap.waterfall_plot(shap.Explanation(values=shap_values_xgb_ohe_bg[c][row], \n",
    "                                        base_values=explainer_xgb_ohe_bg.expected_value[c], \n",
    "                                        data=X_test_data_ohe.iloc[row],  \n",
    "                                        feature_names=X_test_data_ohe.columns.tolist()), \n",
    "                                        show=False)\n",
    "    ax. set_title(f\"Predicted discharge mRS {c}\")\n",
    "    # Increase subplot counter\n",
    "    count+=1\n",
    "    \n",
    "plt.gcf().set_size_inches(20,15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the 3D numpy array (shap_values) match the format required by shap.summary_plot - a list of 7 arrays (shap_values_list).\n",
    "\n",
    "Only include the number of features want for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_display = 0\n",
    "end_display = 5\n",
    "\n",
    "shap_values_list = []\n",
    "for i in range(n_classes):\n",
    "    shap_values_list.append(shap_values[:,st_display:end_display,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "#fig.legend(loc=4)\n",
    "ax = shap.summary_plot(shap_values_list, X_data.iloc[:,st_display:end_display].values, \n",
    "                       plot_type=\"bar\", \n",
    "                       class_names=model.classes_, \n",
    "                       feature_names = X_data.iloc[:,st_display:end_display].columns, \n",
    "                       class_inds=\"original\",\n",
    "                       show=False)\n",
    "#fig.legend(loc=4)\n",
    "#ax.legend(loc=4)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can look at an individual class. Here each subplot shows the results for a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_display = 0\n",
    "end_display = 5\n",
    "\n",
    "shap_values_list = []\n",
    "for i in range(n_classes):\n",
    "    shap_values_list.append(shap_values[:,st_display:end_display,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,50))\n",
    "for i in range(n_classes):  \n",
    "    ax = fig.add_subplot(1,n_classes,i+1)\n",
    "    ax = shap.summary_plot(shap_values_list[i], \n",
    "                           X_data.iloc[:,st_display:end_display].values, \n",
    "                           feature_names=X_data.iloc[:,st_display:end_display].columns, \n",
    "                           show=False, auto_size_plot=False)\n",
    "    #ax.title(f\"Class {model.classes_[i]}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP dependency plot\n",
    "\n",
    "The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 2001 [3]). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. The partial dependence plot is a global method: The method considers all instances and gives a statement about the global relationship of a feature with the predicted outcome. An assumption of the PDP is that the first feature are not correlated with the second feature. If this assumption is violated, the averages calculated for the partial dependence plot will include data points that are very unlikely or even impossible. A dependence plot is a scatter plot that shows the effect a single feature has on the predictions made by the model. In this example the property value increases significantly when the average number of rooms per dwelling is higher than 6. Each dot is a single prediction (row) from the dataset. The x-axis is the actual value from the dataset. The y-axis is the SHAP value for that feature, which represents how much knowing that feature’s value changes the output of the model for that sample’s prediction. The color corresponds to a second feature that may have an interaction effect with the feature we are plotting (by default this second feature is chosen automatically). If an interaction effect is present between this other feature and the feature we are plotting it will show up as a distinct vertical pattern of coloring.\n",
    "\n",
    "## Look at these examples using histograms with dependency plots\n",
    "https://arxiv.org/pdf/2112.11071.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_list = []\n",
    "for i in range(n_classes):\n",
    "    shap_values_list.append(shap_values[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we pass a numpy array instead of a data frame then we\n",
    "# need pass the feature names in separately\n",
    "shap.dependence_plot(ind=features_ohe[0], \n",
    "                     interaction_index=features_ohe[1], \n",
    "                     shap_values=shap_values_list[0], features=X_data.values, \n",
    "                     feature_names=X_data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add histograms to the outside of the dependence plot\n",
    "\n",
    "https://www.nature.com/articles/s41598-021-99920-7\n",
    "https://stackoverflow.com/questions/74859375/shap-partial-dependence-plots-scatter-plot-regression-line-histogram\n",
    "https://arxiv.org/pdf/2112.11071"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need new package: statsmodels. Tried to install it following these info\n",
    "https://www.statsmodels.org/dev/install.html\n",
    "\n",
    "python -m pip install statsmodels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import statsmodels.api as sm\n",
    "\n",
    "var_one = \"prior_disability\"\n",
    "var_two = \"stroke_severity\"\n",
    "plot_class = 0\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "gs = gridspec.GridSpec(5, 5)\n",
    "ax_main = plt.subplot(gs[1:5, :4])\n",
    "ax_xDist = plt.subplot(gs[0, :4])\n",
    "ax_yDist = plt.subplot(gs[1:5, 4])\n",
    "  \n",
    "# lowess\n",
    "idx = np.where(X_data.columns==var_one)[0][0]\n",
    "x = X_data.iloc[:,idx]\n",
    "y_sv = shap_values_list[plot_class][:,idx]\n",
    "lowess = sm.nonparametric.lowess(y_sv, x, frac=.3)\n",
    "ax_main.plot(*list(zip(*lowess)), color=\"#312D2C\", linestyle=\"dashed\", )\n",
    "\n",
    "# shap\n",
    "shap.dependence_plot(var_one,\n",
    "                     shap_values_list[plot_class],\n",
    "                     X_data,\n",
    "                     interaction_index=var_two,\n",
    "                     alpha=0.5,\n",
    "                     dot_size=10,\n",
    "                     show=False,\n",
    "                     ax=ax_main)\n",
    "\n",
    "# histplots\n",
    "ax_xDist.hist(X_data[var_one], bins=50, edgecolor=\"black\", color=\"gray\")\n",
    "ax_yDist.hist(X_data[var_two], orientation=\"horizontal\", bins=50, edgecolor=\"black\", color=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(4):\n",
    "    print(f'Feature {features_ohe[f]} has 5%-95% range: {np.nanpercentile(X_data[features_ohe[f]], 5)} to {np.nanpercentile(X_data[features_ohe[f]], 95)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "gs = gridspec.GridSpec(5, 5)\n",
    "ax_main = plt.subplot(gs[1:5, :4])\n",
    "ax_xDist = plt.subplot(gs[0, :3])\n",
    "ax_yDist = plt.subplot(gs[1:5, 4])\n",
    "  \n",
    "# If we pass a numpy array instead of a data frame then we\n",
    "# need pass the feature names in separately\n",
    "shap.dependence_plot(ind=features_ohe[0], \n",
    "                     interaction_index=features_ohe[1], \n",
    "                     shap_values=shap_values_list[0], features=X_data.values, \n",
    "                     feature_names=X_data.columns, alpha=0.5,\n",
    "                     show=False, ax=ax_main)\n",
    "\n",
    "# Want to change colourmap to be full range of stroke severity (at the mo it's set to 5% and 95% percentiles)\n",
    "# https://github.com/slundberg/shap/issues/695\n",
    "#                     xmin=X_data[features_ohe[1]].min(), xmax=X_data[features_ohe[1]].max())\n",
    "#                     xmin=\"percentile(0)\", xmax=\"percentile(100)\")#dot_size=10, \n",
    "\n",
    "# histplots\n",
    "ax_xDist.hist(X_data[features_ohe[0]], bins=50, edgecolor=\"black\", color=\"gray\")\n",
    "ax_yDist.hist(X_data[features_ohe[1]], orientation=\"horizontal\", bins=50, edgecolor=\"black\", color=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_display = 4\n",
    "\n",
    "# Create a matrix of subplots per class. Each showing the relationship between\n",
    "# each combination of features on the SHAP value.\n",
    "for c in range(n_classes):\n",
    "    # setup matrix of subplots\n",
    "    fig, axes = plt.subplots(nrows=max_display,\n",
    "                             ncols=max_display)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Set overall title\n",
    "    fig.suptitle(f'Class {model.classes_[c]}', fontsize=30)\n",
    "\n",
    "    # Initialise subplot counter\n",
    "    count = 0\n",
    "\n",
    "    # Loop through the features to display\n",
    "    for i in range(max_display):\n",
    "        # Loop through the features to display\n",
    "        for j in range(max_display):\n",
    "            # Create the plot. Pass the axes\n",
    "            shap.dependence_plot(ind=features_ohe[i], \n",
    "                                interaction_index=features_ohe[j], \n",
    "                                shap_values=shap_values_list[c], \n",
    "                                features=X_data.values, \n",
    "                                feature_names=X_data.columns,\n",
    "                                show=False, ax=axes[count],\n",
    "                                alpha=0.1)#, dot_size=2)\n",
    "            # reduce the dot size/transparancy to reveal dense vs sparse areas\n",
    "            #https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html\n",
    "            \n",
    "            # Add line as shap=0\n",
    "            axes[count].plot([-1, X_data[features_ohe[i]].max()+1],\n",
    "                             [0,0],c='0.5')\n",
    "            \n",
    "            # Increase subplot counter\n",
    "            count+=1\n",
    "    \n",
    "    # Change font size for each subplot\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=15)\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=15)\n",
    "        ax.tick_params(axis='both',which='major',labelsize=15)\n",
    "\n",
    "    # Visual propoerties of figure\n",
    "    dimension = 5 * 5\n",
    "    fig.set_figheight(dimension)\n",
    "    fig.set_figwidth(dimension)\n",
    "    fig.tight_layout(pad=2)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Force plot\n",
    "\n",
    "Force plot gives us the explainability of a single model prediction. In this plot we can see how features contributed to the model’s prediction for a specific observation. It is very convenient to use for error analysis or for a deep understanding of a particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 90\n",
    "plot_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[plot_class], shap_values_list[plot_class][row], \n",
    "                X_data.values[row], feature_names = X_data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP waterfall plot\n",
    "\n",
    "Waterfall is another local analysis plot of a single instance prediction, for a single outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.waterfall_plot(shap.Explanation(values=shap_values_list[plot_class][row], \n",
    "                                        base_values=explainer.expected_value[plot_class], data=X_data.iloc[row],  \n",
    "                                        feature_names=X_data.columns.tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the waterfall plots for all of the outcome classes for a single instance.\n",
    "\n",
    "\n",
    "Create a function to create a set of subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_waterfall_multiclass_grid(row, y_data_row, n_classes, shap_values_list_row,\n",
    "                                     base_values, data, feature_names):\n",
    "\n",
    "    # Create a matrix of subplots per class. Each showing the relationship between\n",
    "    # each combination of features on the SHAP value.\n",
    "    # setup matrix of subplots\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Set overall title\n",
    "    fig.suptitle(f'Waterfall for instance {row} '\n",
    "                f'(observed dischange mRS = {y_data_row})', \n",
    "                fontsize=30)\n",
    "\n",
    "    # Initialise subplot counter\n",
    "    count = 1\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        ax = fig.add_subplot(4,2,count)\n",
    "        shap.waterfall_plot(shap.Explanation(values=shap_values_list_row[c], \n",
    "                                            base_values=base_values[c], \n",
    "                                            data=data,  \n",
    "                                            feature_names=feature_names), \n",
    "                                            show=False)\n",
    "        ax. set_title(f\"Predicted discharge mRS {c}\")\n",
    "        # Increase subplot counter\n",
    "        count+=1\n",
    "        \n",
    "    plt.gcf().set_size_inches(20,15)\n",
    "    plt.tight_layout()\n",
    "    # Change font size for each subplot\n",
    "    #for ax in axes:\n",
    "    #    ax.set_xlabel(ax.get_xlabel(), fontsize=15)\n",
    "    #    ax.set_ylabel(ax.get_ylabel(), fontsize=15)\n",
    "    #    ax.tick_params(axis='both',which='major',labelsize=15)\n",
    "\n",
    "    # Visual propoerties of figure\n",
    "    #dimension = 5 * 5\n",
    "    #fig.set_figheight(dimension)\n",
    "    #fig.set_figwidth(dimension)\n",
    "    #fig.tight_layout(pad=2)\n",
    "    plt.show()\n",
    "\n",
    "    return()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View waterfall plots for all classes for a single instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 1\n",
    "\n",
    "# Through each array in the list and extract just the row want to plot (put that back in the list)\n",
    "shap_values_list_row=[]\n",
    "for a in shap_values_list:\n",
    "    shap_values_list_row.append(a[row,:])\n",
    "\n",
    "# Call funciton to create subplots\n",
    "create_waterfall_multiclass_grid(row, y_data.iloc[row], n_classes, \n",
    "                                 shap_values_list_row,\n",
    "                                 explainer.expected_value, X_data.iloc[row], \n",
    "                                 X_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 0\n",
    "\n",
    "# Through each array in the list and extract just the row want to plot (put that back in the list)\n",
    "shap_values_list_row=[]\n",
    "for a in shap_values_list:\n",
    "    shap_values_list_row.append(a[row,:])\n",
    "\n",
    "# Call function to create subplots\n",
    "create_waterfall_multiclass_grid(row, y_data.iloc[row], n_classes, \n",
    "                                 shap_values_list_row,\n",
    "                                 explainer.expected_value, X_data.iloc[row], \n",
    "                                 X_data.columns.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the violin plots as used for the SHAP paper\n",
    "\n",
    "Violin plots show the relationship between feature value and SHAP value for all of the features\n",
    "Output descriptive text to use in the paper to describe the differences a feature value had on the likelihood of receiving thromboylsis.\n",
    "\n",
    "Resource: https://towardsdatascience.com/binning-records-on-a-continuous-variable-with-pandas-cut-and-qcut-5d7c8e11d7b0 https://matplotlib.org/3.1.0/gallery/statistics/customized_violin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ax(ax, category_list, feat, rotation=0):\n",
    "    '''\n",
    "    ax [matplotlib axis object] = matplotlib axis object\n",
    "    category_list [list] = used for the xtick labels (the grouping of the data)\n",
    "    feat [string] = used in the axis label, the feature that is being plotted\n",
    "    rotation [integer] = xtick label rotation\n",
    "\n",
    "    reeturn [matplotlib axis object]\n",
    "    \n",
    "    resource: \n",
    "    https://matplotlib.org/3.1.0/gallery/statistics/customized_violin.html\n",
    "    '''\n",
    "    # Set the axes ranges and axes labels\n",
    "    ax.get_xaxis().set_tick_params(direction='out')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.set_xticks(np.arange(1, len(category_list) + 1))\n",
    "    ax.set_xticklabels(category_list, rotation=rotation, fontsize=7)\n",
    "    ax.set_xlim(0.25, len(category_list) + 0.75)\n",
    "    ax.set_ylabel(f'SHAP values for {feat}', fontsize=12)\n",
    "    ax.set_xlabel(f'Feature values for {feat}', fontsize=12)\n",
    "    return(ax)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create violin plot and descriptive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to include in the violin plot grid\n",
    "features_violin = ['prior_disability','stroke_severity',\n",
    "                     'onset_to_thrombolysis_time','age']\n",
    "\n",
    "class_category = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'onset_to_thrombolysis_time'\n",
    "class_category = 6\n",
    "\n",
    "feature_data = shap_values_extended[:, feat, class_category].data\n",
    "feature_shap = shap_values_extended[:, feat, class_category].values\n",
    "\n",
    "# If feature has more that 50 unique values, then assume it needs to be \n",
    "# binned (otherwise assume they are unique categories)\n",
    "\n",
    "# bin the data, create a violin per bin\n",
    "\n",
    "# settings for the plot\n",
    "rotation = 45\n",
    "step = 30\n",
    "n_bins = min(11, np.int((feature_data.max())/step))\n",
    "\n",
    "# create list of bin values\n",
    "bin_list = [(i*step) for i in range(n_bins)]\n",
    "bin_list.append(feature_data.max())\n",
    "if feat == \"onset_to_thrombolysis_time\":\n",
    "    # for -100 for no IVT\n",
    "    bin_list.insert(0, feature_data.min())\n",
    "\n",
    "bin_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of instances per bin\n",
    "#counts, edges, plot = plt.hist(feature_data, bins=bin_list)\n",
    "counts, edges = np.histogram(feature_data, bins=bin_list, density=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(feature_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_of_violin_plots(features_violin, \n",
    "                                class_category,\n",
    "                                shap_values_extended\n",
    "                                ):\n",
    "    # Feature Age needs special consideration. It needs the x ticks to be created,\n",
    "    # as the other features with over 50 unique values, but age is already grouped \n",
    "    # for the model (into 5yr groups) and so is treated as the other type.\n",
    "\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(12,14), constrained_layout=True)\n",
    "\n",
    "\n",
    "    fig.suptitle(f'Model to predict discharge disability mRS = {class_category}', fontsize=20)\n",
    "\n",
    "    # A subplot showing violin plot for each feature.\n",
    "    # First prepare the fature data for the violin plot: if feature has more than \n",
    "    # 50 unique values then assume it needs to be binned (a violin for each bin)\n",
    "\n",
    "    # Determine number of rows of subplots by rounding up\n",
    "    ncols = 2\n",
    "    nrows = math.ceil(len(features_violin)/ncols)\n",
    "\n",
    "    # Through each feature\n",
    "    for n, feat in enumerate(features_violin):    \n",
    "        \n",
    "        # Get data and SHAP values\n",
    "        feature_data = shap_values_extended[:, feat, class_category].data\n",
    "        feature_shap = shap_values_extended[:, feat, class_category].values\n",
    "\n",
    "        # If feature has more that 50 unique values, then assume it needs to be \n",
    "        # binned (otherwise assume they are unique categories)\n",
    "\n",
    "        if np.unique(feature_data).shape[0] > 50:\n",
    "            # bin the data, create a violin per bin\n",
    "            \n",
    "            # settings for the plot\n",
    "            rotation = 45\n",
    "            step = 30\n",
    "            n_bins = min(11, np.int((feature_data.max())/step))\n",
    "            \n",
    "            # create list of bin values\n",
    "            bin_list = [(i*step) for i in range(n_bins)]\n",
    "            bin_list.append(feature_data.max())\n",
    "            if feat == \"onset_to_thrombolysis_time\":\n",
    "                # for -100 for no IVT\n",
    "                bin_list.insert(0, feature_data.min())\n",
    "\n",
    "            # Number of instances per bin\n",
    "            #counts, edges, plot = plt.hist(feature_data, bins=bin_list)\n",
    "            counts, edges = np.histogram(feature_data, bins=bin_list, density=False)\n",
    "\n",
    "            # create list of bins (the unique categories)\n",
    "            category_list = [f'{i*step}-{((i+1)*step-1)}\\n[n={int(counts[i+1])}]' for i in range(n_bins-1)]\n",
    "            category_list.append(f'{(n_bins-1)*step}+\\n[n={int(counts[-1])}]')\n",
    "            if feat == \"onset_to_thrombolysis_time\":\n",
    "                # for -100 for no IVT\n",
    "                category_list.insert(0, f'Not receive IVT\\n[n={int(counts[0])}]')\n",
    "\n",
    "            # bin the feature data\n",
    "            feature_data = pd.cut(feature_data, bin_list, labels=category_list, \n",
    "                                right=False)\n",
    "\n",
    "            # create a list, each entry contains the corresponsing SHAP value for that \n",
    "            # category (or bin). A violin will represent each list.    \n",
    "            shap_per_category = []\n",
    "            for category in category_list:\n",
    "                mask = feature_data == category\n",
    "                shap_per_category.append(feature_shap[mask])\n",
    "\n",
    "        else:\n",
    "            # create a violin per unique value\n",
    "            \n",
    "            # settings for the plot\n",
    "            rotation = 90\n",
    "            \n",
    "            # create list of unique categories in the feature data\n",
    "            category_list = np.unique(feature_data)\n",
    "\n",
    "            # Age needs to keep its decimal value (midpoint between 5 yrs)\n",
    "            if feat != 'age':\n",
    "                category_list = [int(i) for i in category_list]\n",
    "\n",
    "            # create a list, each entry contains the corresponsing SHAP value for that \n",
    "            # category (or bin). A violin will represent each list.    \n",
    "            count = []\n",
    "            shap_per_category = []\n",
    "            for category in category_list:\n",
    "                mask = feature_data == category\n",
    "                shap_per_category.append(feature_shap[mask])\n",
    "                count.append(mask.sum())\n",
    "#                if feat == 'stroke_severity':\n",
    "#                    print(feature_shap[mask].max())\n",
    "\n",
    "            if feat == 'age':\n",
    "                # create text of x ticks\n",
    "                category_list = [f'{int(i-2.5)}-{int(i+2.5)}' for i in category_list]\n",
    "\n",
    "                # SSNAP dataset had oldest age category as 100-120 (not a 5 yr band as \n",
    "                #   the other ages). To accommodate this, if last age category is \"110\"\n",
    "                #   then overwrite the label with the correct band (100-120), and not\n",
    "                #   107-112 as the above code would create.\n",
    "                if category_list[-1] == '107-112':\n",
    "                    category_list[-1] = '100-120'\n",
    "            if feat == 'stroke_severity':\n",
    "                # not enough space of chart for 2 lines fo x tick labels\n",
    "                category_list = [f'{category_list[i]} [n={count[i]}]' for i in range(len(category_list))]\n",
    "            else:\n",
    "                category_list = [f'{category_list[i]}\\n[n={count[i]}]' for i in range(len(category_list))]\n",
    "\n",
    "        # create violin plot\n",
    "        ax = fig.add_subplot(nrows,ncols,n+1)\n",
    "\n",
    "        ax.violinplot(shap_per_category, showmeans=True, widths=0.9)\n",
    "\n",
    "        # Add line at Shap = 0\n",
    "        feature_values = shap_values_extended[:, feat].data\n",
    "        ax.plot([0, len(feature_values)], [0,0],c='0.5')   \n",
    "\n",
    "        # customise the axes\n",
    "        ax = set_ax(ax, category_list, feat, rotation=rotation)\n",
    "        plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
    "\n",
    "        # Adjust stroke severity tickmarks\n",
    "        if feat == 'Stroke severity':\n",
    "            ax.set_xticks(np.arange(1, len(category_list)+1, 2))\n",
    "            ax.set_xticklabels(category_list[0::2])   \n",
    "            \n",
    "        # Add title\n",
    "        ax.set_title(feat)\n",
    "        \n",
    "    plt.tight_layout(pad=2)\n",
    "        \n",
    "    fig.savefig(f'{paths.notebook}{paths.model_text}_thrombolysis_shap_violin_all_'\n",
    "                f'features_for_mRS{class_category}.jpg', \n",
    "                dpi=300, bbox_inches='tight', pad_inches=0.2)\n",
    "\n",
    "    plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_category in range(n_classes):\n",
    "    create_grid_of_violin_plots(features_violin, \n",
    "                                class_category,\n",
    "                                shap_values_extended\n",
    "                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some of the violins for class 6 as multiple patient populations.\n",
    "\n",
    "1) Feature onset to thrombolysis time, divide by stroke severity (using 10 as the split).\n",
    "2) Feature onset to thrombolysis time, divide by stroke severity (using 5, 10, 25 as the splits)\n",
    "3) Feature onset to thrombolysis time, divide by stroke severity (using 5 as the split)\n",
    "4) Feature onset to thrombolysis time, divide by stroke severity (using Wikipedia stroke severity split: https://en.wikipedia.org/wiki/National_Institutes_of_Health_Stroke_Scale)\n",
    "5) Feature stroke severity, divide by get IVT or not get IVT\n",
    "6) Feature stroke severity, divide by get IVT early (<2hrs), get IVT late (>2hrs) or not get IVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_subpopulations(list_masks, list_titles, feature_data, feature_shap, \n",
    "                          rotation, bin_list=bin_list, step=step, n_bins=n_bins): \n",
    "                          #,ymin=ymin,ymax=ymax):\n",
    "\n",
    "    # A subplot showing violin plot for each feature.\n",
    "    # First prepare the fature data for the violin plot: if feature has more than \n",
    "    # 50 unique values then assume it needs to be binned (a violin for each bin)\n",
    "\n",
    "    # Initialise vlaues so they can be immediately improved. Stores the overall min and max of y axis ranges for all subplots.\n",
    "    ymin = 999\n",
    "    ymax = -999\n",
    "\n",
    "    # Determine number of rows of subplots by rounding up\n",
    "    ncols = min(len(list_masks),2)\n",
    "    nrows = int(math.ceil(len(list_masks)/ncols))\n",
    "\n",
    "    figwidth = 12\n",
    "    figheight = 7 * nrows\n",
    "\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(figwidth,figheight), constrained_layout=True)\n",
    "\n",
    "    fig.suptitle(f'Model to predict discharge disability mRS = {class_category}', fontsize=20)\n",
    "\n",
    "\n",
    "    plot_mean_shap_per_category = []\n",
    "    plot_median_shap_per_category = []\n",
    "    plot_sem_shap_per_category = []\n",
    "\n",
    "    for i in range(len(list_titles)):\n",
    "        feature_data_plot = feature_data[list_masks[i]]\n",
    "        feature_shap_plot = feature_shap[list_masks[i]]\n",
    "\n",
    "        if np.unique(feature_data).shape[0] > 50:            \n",
    "            # Number of instances per bin\n",
    "            #counts, edges, plot = plt.hist(feature_data, bins=bin_list)\n",
    "            counts, edges = np.histogram(feature_data_plot, bins=bin_list, density=False)\n",
    "\n",
    "            # create list of bins (the unique categories)\n",
    "            category_list = [f'{i*step}-{((i+1)*step-1)}\\n[n={int(counts[i+1])}]' for i in range(n_bins-1)]\n",
    "            category_list.append(f'{(n_bins-1)*step}+\\n[n={int(counts[-1])}]')\n",
    "            if feat == \"onset_to_thrombolysis_time\":\n",
    "                # for -100 for no IVT\n",
    "                category_list.insert(0, f'Not receive IVT\\n[n={int(counts[0])}')\n",
    "\n",
    "            # bin the feature data\n",
    "            feature_data_plot = pd.cut(feature_data_plot, bin_list, labels=category_list, \n",
    "                                right=False)\n",
    "\n",
    "        else:\n",
    "            category_list = np.unique(feature_data)\n",
    "\n",
    "        # create a list, each entry contains the corresponsing SHAP value for that \n",
    "        # category (or bin). A violin will represent each list.    \n",
    "        shap_per_category = []\n",
    "        mean_shap_per_category = []\n",
    "        median_shap_per_category = []\n",
    "        sem_shap_per_category = []\n",
    "        if np.unique(feature_data).shape[0] < 51:            \n",
    "            count = []\n",
    "\n",
    "        for category in category_list:\n",
    "            mask = feature_data_plot == category\n",
    "            shap_per_category.append(feature_shap_plot[mask])\n",
    "            mean = feature_shap_plot[mask].mean()\n",
    "            mean_shap_per_category.append(mean)\n",
    "            median = np.median(feature_shap_plot[mask])\n",
    "            median_shap_per_category.append(median)\n",
    "            stddev = np.std(feature_shap_plot[mask])\n",
    "            sem_shap_per_category.append(stddev/math.sqrt(mask.sum()))\n",
    "            if np.unique(feature_data).shape[0] < 51:            \n",
    "                count.append(mask.sum())\n",
    "\n",
    "        if np.unique(feature_data).shape[0] < 51:       \n",
    "            if feat == 'stroke_severity':\n",
    "                # not enough space of chart for 2 lines fo x tick labels\n",
    "                category_list = [f'{category_list[i]} [n={count[i]}]' for i in range(len(category_list))]\n",
    "            else:\n",
    "                category_list = [f'{category_list[i]}\\n[n={count[i]}]' for i in range(len(category_list))]\n",
    "\n",
    "        # create violin plot\n",
    "        ax = fig.add_subplot(nrows,ncols,i+1)\n",
    "\n",
    "        # https://stackoverflow.com/questions/33822239/matplotlib-violinplot-valueerror-with-empty-lists\n",
    "        nans = [float('nan'), float('nan')] # requires at least 2 nans\n",
    "\n",
    "        ax.violinplot([val if val.any() else nans for val in shap_per_category], \n",
    "                  showmeans=True, widths=0.9)\n",
    "    \n",
    "    #    ax.violinplot(shap_per_category, showmeans=True, widths=0.9)\n",
    "\n",
    "        # Add line at Shap = 0\n",
    "        feature_values = shap_values_extended[:, feat].data\n",
    "        ax.plot([0, len(feature_values)], [0,0],c='0.5')   \n",
    "\n",
    "        # customise the axes\n",
    "        ax = set_ax(ax, category_list, feat, rotation=rotation)\n",
    "        plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
    "#        ax.set_ylim(-1, 1.75)\n",
    "        ymin = min(ax.get_ylim()[0],ymin)\n",
    "        ymax = max(ax.get_ylim()[1], ymax)\n",
    "        \n",
    "        # Add title\n",
    "        ax.set_title(list_titles[i])\n",
    "        plot_mean_shap_per_category.append(mean_shap_per_category)\n",
    "        plot_median_shap_per_category.append(median_shap_per_category)\n",
    "        plot_sem_shap_per_category.append(sem_shap_per_category)\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "    # Use the same y limits for all of the subplots\n",
    "    # To use the range from the first subplot use: ylim=ax[0,0].get_ylim()\n",
    "    plt.setp(ax, ylim=(ymin,ymax))\n",
    "\n",
    "    #fig.savefig(f'{paths.notebook}_{paths.model_text}_thrombolysis_shap_violin_all_'\n",
    "    #        f'features_for_mRS{class_category}.jpg', \n",
    "    #        dpi=300, bbox_inches='tight', pad_inches=0.2)\n",
    "\n",
    "    plt.show()\n",
    "    return(plot_mean_shap_per_category, plot_median_shap_per_category, plot_sem_shap_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineplot_subpopulations(plot_mean_shap_per_category, plot_sem_shap_per_category, list_titles,n_bins=n_bins, step=step):\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(10,5), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    if np.unique(feature_data).shape[0] < 50:            \n",
    "        category_list = np.unique(feature_data)\n",
    "\n",
    "    else:\n",
    "\n",
    "        category_list = [f'{i*step}-{((i+1)*step-1)}' for i in range(n_bins-1)]\n",
    "        category_list.append(f'{(n_bins-1)*step}+')\n",
    "        if feat == \"onset_to_thrombolysis_time\":\n",
    "                # for -100 for no IVT\n",
    "                category_list.insert(0, f'Not receive IVT')\n",
    "\n",
    "\n",
    "    for i in range(len(plot_mean_shap_per_category)):\n",
    "        ax.errorbar(range(1,len(plot_mean_shap_per_category[i])+1), \n",
    "                plot_mean_shap_per_category[i],\n",
    "                label=list_titles[i],\n",
    "                yerr=plot_sem_shap_per_category[i])\n",
    "    #ax.plot(range(43), plot_mean_shap_per_category[1])\n",
    "\n",
    "    # Add line at Shap = 0\n",
    "    ax.plot([0, len(plot_mean_shap_per_category[0])], [0,0],c='0.5')   \n",
    "\n",
    "    # customise the axes\n",
    "    ax = set_ax(ax, category_list, feat, rotation=rotation)\n",
    "    plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
    "    #ax.set_ylim(-1.9, 4.5)\n",
    "\n",
    "    # Add title\n",
    "    ax.set_title(\"Mean SHAP (sem) for feature onset to thrombolysis time, by stroke severity\")\n",
    "\n",
    "    ax.legend(loc=4)\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "    #fig.savefig(f'{paths.notebook}_{paths.model_text}_thrombolysis_shap_violin_all_'\n",
    "    #        f'features_for_mRS{class_category}.jpg', \n",
    "    #        dpi=300, bbox_inches='tight', pad_inches=0.2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_category = 6\n",
    "feat = \"onset_to_thrombolysis_time\"\n",
    "\n",
    "# Get data and SHAP values\n",
    "feature_data = shap_values_extended[:, feat, class_category].data\n",
    "feature_shap = shap_values_extended[:, feat, class_category].values\n",
    "\n",
    "# settings for the plot\n",
    "rotation = 45\n",
    "step = 30\n",
    "n_bins = min(11, np.int((feature_data.max())/step))\n",
    "\n",
    "# create list of bin values\n",
    "bin_list = [(i*step) for i in range(n_bins)]\n",
    "bin_list.append(feature_data.max())\n",
    "# for -100 for no IVT\n",
    "bin_list.insert(0, feature_data.min())\n",
    "\n",
    "# Divide based on stroke severity.\n",
    "stroke_severity_data = shap_values_extended[:, \"stroke_severity\", class_category].data\n",
    "list_masks = []\n",
    "mask = stroke_severity_data < 11\n",
    "list_masks.append(mask)\n",
    "mask = stroke_severity_data > 10\n",
    "list_masks.append(mask)\n",
    "\n",
    "# check each instance is represented exactly once\n",
    "sum = 0\n",
    "for i in list_masks:\n",
    "    sum = sum + i.sum()\n",
    "print(f\"{sum} patients out of {feature_data.shape[0]} total are represented across the graphs\")\n",
    "\n",
    "list_titles = [\"Mild strokes (NIHSS 0-10)\", \"Severe strokes (NIHSS 11+)\"]\n",
    "\n",
    "# Show these populations in violin subplots\n",
    "(plot_mean_shap_per_category, \n",
    " plot_median_shap_per_category, \n",
    " plot_sem_shap_per_category) = violin_subpopulations(list_masks, list_titles, \n",
    "                                                     feature_data, feature_shap,\n",
    "                                                      rotation, bin_list=bin_list, step=step, n_bins=n_bins)\n",
    "\n",
    "\n",
    "lineplot_subpopulations(plot_mean_shap_per_category, plot_sem_shap_per_category, list_titles,n_bins=n_bins, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_category = 6\n",
    "feat = \"onset_to_thrombolysis_time\"\n",
    "\n",
    "# Get data and SHAP values\n",
    "feature_data = shap_values_extended[:, feat, class_category].data\n",
    "feature_shap = shap_values_extended[:, feat, class_category].values\n",
    "\n",
    "# settings for the plot\n",
    "step = 30\n",
    "n_bins = min(11, np.int((feature_data.max())/step))\n",
    "\n",
    "# create list of bin values\n",
    "bin_list = [(i*step) for i in range(n_bins)]\n",
    "bin_list.append(feature_data.max())\n",
    "# for -100 for no IVT\n",
    "bin_list.insert(0, feature_data.min())\n",
    "\n",
    "# Divide based on stroke severity.\n",
    "stroke_severity_data = shap_values_extended[:, \"stroke_severity\", class_category].data\n",
    "list_masks = []\n",
    "mask = stroke_severity_data < 6\n",
    "list_masks.append(mask)\n",
    "mask = (stroke_severity_data > 5) & (stroke_severity_data < 11)\n",
    "list_masks.append(mask)\n",
    "mask = (stroke_severity_data > 10) & (stroke_severity_data < 26)\n",
    "list_masks.append(mask)\n",
    "mask = stroke_severity_data > 25\n",
    "list_masks.append(mask)\n",
    "\n",
    "# check each instance is represented exactly once\n",
    "sum = 0\n",
    "for i in list_masks:\n",
    "    sum = sum + i.sum()\n",
    "print(f\"{sum} patients out of {feature_data.shape[0]} total are represented across the graphs\")\n",
    "\n",
    "list_titles = [\"Mild strokes (NIHSS 0-5)\", \"Low mid strokes (NIHSS 6-10)\", \"High mid strokes (NIHSS 11-25)\", \"Severe strokes (NIHSS 26+)\"]\n",
    "\n",
    "# Show these populations in violin subplots\n",
    "(plot_mean_shap_per_category, \n",
    " plot_median_shap_per_category, \n",
    " plot_sem_shap_per_category) = violin_subpopulations(list_masks, list_titles, \n",
    "                                                     feature_data, feature_shap,\n",
    "                                                      rotation, bin_list=bin_list, step=step, n_bins=n_bins)\n",
    "\n",
    "\n",
    "lineplot_subpopulations(plot_mean_shap_per_category, plot_sem_shap_per_category, list_titles, n_bins=n_bins, step=step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the above four plots on one plot: mean and sem (line per NIHSS category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_category = 6\n",
    "feat = \"onset_to_thrombolysis_time\"\n",
    "\n",
    "# Get data and SHAP values\n",
    "feature_data = shap_values_extended[:, feat, class_category].data\n",
    "feature_shap = shap_values_extended[:, feat, class_category].values\n",
    "\n",
    "# settings for the plot\n",
    "rotation = 45\n",
    "step = 30\n",
    "n_bins = min(11, np.int((feature_data.max())/step))\n",
    "\n",
    "# create list of bin values\n",
    "bin_list = [(i*step) for i in range(n_bins)]\n",
    "bin_list.append(feature_data.max())\n",
    "# for -100 for no IVT\n",
    "bin_list.insert(0, feature_data.min())\n",
    "\n",
    "# Divide based on stroke severity.\n",
    "stroke_severity_data = shap_values_extended[:, \"stroke_severity\", class_category].data\n",
    "list_masks = []\n",
    "mask = stroke_severity_data < 6\n",
    "list_masks.append(mask)\n",
    "mask = stroke_severity_data > 5\n",
    "list_masks.append(mask)\n",
    "\n",
    "# check each instance is represented exactly once\n",
    "sum = 0\n",
    "for i in list_masks:\n",
    "    sum = sum + i.sum()\n",
    "print(f\"{sum} patients out of {feature_data.shape[0]} total are represented across the graphs\")\n",
    "\n",
    "list_titles = [\"Mild strokes (NIHSS 0-5)\", \"Severe strokes (NIHSS 6+)\"]\n",
    "\n",
    "# Show these populations in violin subplots\n",
    "(plot_mean_shap_per_category, \n",
    " plot_median_shap_per_category, \n",
    " plot_sem_shap_per_category) = violin_subpopulations(list_masks, list_titles, \n",
    "                                                     feature_data, feature_shap,\n",
    "                                                      rotation, bin_list=bin_list, step=step, n_bins=n_bins)\n",
    "\n",
    "\n",
    "lineplot_subpopulations(plot_mean_shap_per_category, plot_sem_shap_per_category, list_titles,n_bins=n_bins, step=step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Wikipedia stroke severity split: https://en.wikipedia.org/wiki/National_Institutes_of_Health_Stroke_Scale\n",
    "\n",
    "Score\tStroke severity\n",
    "0   \tNo stroke symptoms\n",
    "1–4\t    Minor stroke\n",
    "5–15\tModerate stroke\n",
    "16–20\tModerate to severe stroke\n",
    "21–42\tSevere stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_category = 6\n",
    "feat = \"onset_to_thrombolysis_time\"\n",
    "\n",
    "# Get data and SHAP values\n",
    "feature_data = shap_values_extended[:, feat, class_category].data\n",
    "feature_shap = shap_values_extended[:, feat, class_category].values\n",
    "\n",
    "# settings for the plot\n",
    "rotation = 45\n",
    "step = 30\n",
    "n_bins = min(11, np.int((feature_data.max())/step))\n",
    "\n",
    "# create list of bin values\n",
    "bin_list = [(i*step) for i in range(n_bins)]\n",
    "bin_list.append(feature_data.max())\n",
    "# for -100 for no IVT\n",
    "bin_list.insert(0, feature_data.min())\n",
    "\n",
    "# Divide based on stroke severity.\n",
    "stroke_severity_data = shap_values_extended[:, \"stroke_severity\", class_category].data\n",
    "list_masks = []\n",
    "mask = stroke_severity_data == 0\n",
    "list_masks.append(mask)\n",
    "mask = (stroke_severity_data > 0) & (stroke_severity_data < 5)\n",
    "list_masks.append(mask)\n",
    "mask = (stroke_severity_data > 4) & (stroke_severity_data < 16)\n",
    "list_masks.append(mask)\n",
    "mask = (stroke_severity_data > 15) & (stroke_severity_data < 21)\n",
    "list_masks.append(mask)\n",
    "mask = stroke_severity_data > 20\n",
    "list_masks.append(mask)\n",
    "list_titles = [\"No stroke symptoms (NIHSS 0)\", \"Minor stroke (NIHSS 1-4)\",\n",
    "               \"Moderate stroke (NIHSS 5-15)\",\n",
    "               \"Moderate to severe stroke (NIHSS 16-20)\", \n",
    "               \"Severe stroke (NIHSS 21+)\"]\n",
    "\n",
    "# check each instance is represented exactly once\n",
    "sum = 0\n",
    "for i in list_masks:\n",
    "    sum = sum + i.sum()\n",
    "print(f\"{sum} patients out of {feature_data.shape[0]} total are represented across the graphs\")\n",
    "\n",
    "# Show these populations in violin subplots\n",
    "(plot_mean_shap_per_category, \n",
    " plot_median_shap_per_category, \n",
    " plot_sem_shap_per_category) = violin_subpopulations(list_masks, list_titles, \n",
    "                                                     feature_data, feature_shap,\n",
    "                                                     rotation, bin_list=bin_list, \n",
    "                                                     step=step, n_bins=n_bins)\n",
    "\n",
    "lineplot_subpopulations(plot_mean_shap_per_category, plot_sem_shap_per_category, \n",
    "                        list_titles, n_bins=n_bins, step=step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replot the dependency plot of stroke severity for class mRS=6 (death) as a violin plot, and have a violin per stroke severity category, with a graph for those that get IVT, and those that do not get IVT.\n",
    "\n",
    "(Relationship between Stroke Severity and death, with and without IVT)\n",
    "\n",
    "Each violin is a severity. Shap value for stroke severity (which is it's contribution for death when take the class 6 output).\n",
    "\n",
    "Split data by IVT and not IVT. \n",
    "\n",
    "Also plot as two lines - the mean & standard error of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_category = 6\n",
    "feat = \"stroke_severity\"\n",
    "\n",
    "# Get data and SHAP values\n",
    "feature_data = shap_values_extended[:, feat, class_category].data\n",
    "feature_shap = shap_values_extended[:, feat, class_category].values\n",
    "\n",
    "# settings for the plot\n",
    "rotation = 90\n",
    "\n",
    "# Divide based on receive IVT.\n",
    "thrombolysis_data = shap_values_extended[:, \"onset_to_thrombolysis_time\", class_category].data\n",
    "# Get IVT\n",
    "list_masks = []\n",
    "mask = thrombolysis_data > -100\n",
    "list_masks.append(mask)\n",
    "# Not get IVT\n",
    "mask = thrombolysis_data == -100\n",
    "list_masks.append(mask)\n",
    "\n",
    "# check each instance is represented exactly once\n",
    "sum = 0\n",
    "for i in list_masks:\n",
    "    sum = sum + i.sum()\n",
    "print(f\"{sum} patients out of {feature_data.shape[0]} total are represented across the graphs\")\n",
    "\n",
    "list_titles = [\"Get thrombolysis\", \"Not get thrombolysis\"]\n",
    "\n",
    "\n",
    "# Show these populations in violin subplots\n",
    "(plot_mean_shap_per_category, \n",
    " plot_median_shap_per_category, \n",
    " plot_sem_shap_per_category) = violin_subpopulations(list_masks, list_titles, \n",
    "                                                     feature_data, feature_shap,\n",
    "                                                      rotation, bin_list=bin_list, \n",
    "                                                      step=step, n_bins=n_bins)\n",
    "\n",
    "\n",
    "lineplot_subpopulations(plot_mean_shap_per_category, plot_sem_shap_per_category, \n",
    "                        list_titles, n_bins=n_bins, step=step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide get thrombolysis into those that get it early, and those that get it late. Using 2 hours as the cut off point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_category = 6\n",
    "feat = \"stroke_severity\"\n",
    "\n",
    "\n",
    "# A subplot showing violin plot for each feature.\n",
    "# First prepare the feature data for the violin plot: if feature has more than \n",
    "# 50 unique values then assume it needs to be binned (a violin for each bin)\n",
    "\n",
    "# Get data and SHAP values\n",
    "feature_data = shap_values_extended[:, feat, class_category].data\n",
    "feature_shap = shap_values_extended[:, feat, class_category].values\n",
    "\n",
    "# settings for the plot\n",
    "rotation = 90\n",
    "step = 30\n",
    "n_bins = min(11, np.int((feature_data.max())/step))\n",
    "\n",
    "# create list of bin values\n",
    "bin_list = [(i*step) for i in range(n_bins)]\n",
    "bin_list.append(feature_data.max())\n",
    "# for -100 for no IVT\n",
    "bin_list.insert(0, feature_data.min())\n",
    "\n",
    "# Divide based on receive IVT.\n",
    "thrombolysis_data = shap_values_extended[:, \"onset_to_thrombolysis_time\", class_category].data\n",
    "list_masks = []\n",
    "# Get IVT early (within 2 hours)\n",
    "mask = (thrombolysis_data > -100) & (thrombolysis_data < 120)\n",
    "list_masks.append(mask)\n",
    "# Get IVT later (after 2 hours)\n",
    "mask = thrombolysis_data > 119\n",
    "list_masks.append(mask)\n",
    "# Not get IVT\n",
    "mask = thrombolysis_data == -100\n",
    "list_masks.append(mask)\n",
    "\n",
    "# check each instance is represented exactly once\n",
    "sum = 0\n",
    "for i in list_masks:\n",
    "    sum = sum + i.sum()\n",
    "print(f\"{sum} patients out of {feature_data.shape[0]} total are represented across the graphs\")\n",
    "\n",
    "list_titles = [\"Get thrombolysis early (within 2 hours onset)\", \n",
    "               \"Get thrombolysis later (after 2 hours onset)\", \n",
    "               \"Not get thrombolysis\"]\n",
    "\n",
    "# Show these populations in violin subplots\n",
    "(plot_mean_shap_per_category, \n",
    " plot_median_shap_per_category, \n",
    " plot_sem_shap_per_category) = violin_subpopulations(list_masks, list_titles, \n",
    "                                                     feature_data, feature_shap,\n",
    "                                                     rotation, bin_list=bin_list, step=step, n_bins=n_bins)\n",
    "\n",
    "\n",
    "lineplot_subpopulations(plot_mean_shap_per_category, plot_sem_shap_per_category, list_titles,n_bins=n_bins, step=step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare SHAP values for the hospital features\n",
    "\n",
    "The hospital feature is a coded value per team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram_data_and_plot(data_stroke_team,\n",
    "                                   shap_values,\n",
    "                                   features,\n",
    "                                   class_category):\n",
    "\n",
    "    index = features.index(\"stroke_team\")\n",
    "\n",
    "    # Set up list for storing patient data and hospital SHAP\n",
    "    feature_data_with_shap = []\n",
    "\n",
    "    # Get mean SHAP for stroke team when patient attending that stroke team\n",
    "    unique_stroketeams_list = list(np.unique(data_stroke_team))\n",
    "    stroke_team_mean_shap = []\n",
    "    # Loop through stroke teams\n",
    "    for stroke_team in unique_stroketeams_list:\n",
    "        # Identify rows in test data that match each stroke team\n",
    "        mask = data_stroke_team == stroke_team\n",
    "        # Extract hospital SHAP for the rows that attend that hosptial, for the class\n",
    "        stroke_team_shap = shap_values[mask,index,class_category]#shap_values[mask]\n",
    "        # Get mean\n",
    "        mean_shap = np.mean(stroke_team_shap)\n",
    "        # Store mean\n",
    "        stroke_team_mean_shap.append(mean_shap)\n",
    "        # Get and store feature data and add SHAP\n",
    "        feature_data = data[mask]\n",
    "        feature_data['Hospital_SHAP'] = stroke_team_shap\n",
    "        feature_data_with_shap.append(feature_data)\n",
    "\n",
    "    # Concatenate and save feature_data_with_shap\n",
    "    feature_data_with_shap = pd.concat(feature_data_with_shap, axis=0)\n",
    "    feature_data_with_shap.to_csv(\n",
    "    f'./{paths.notebook}{paths.model_text}_feature_data_with_hospital_shap.csv', \n",
    "        index=False)\n",
    "\n",
    "    # Create and save shap mean value per hospital\n",
    "    hospital_data = pd.DataFrame()\n",
    "    hospital_data[\"stroke_team\"] = unique_stroketeams_list\n",
    "    hospital_data[\"shap_mean\"] = stroke_team_mean_shap\n",
    "    hospital_data.to_csv(\n",
    "        f'./{paths.notebook}{paths.model_text}_mean_shap_per_hospital_for_mRS{class_category}.csv', \n",
    "        index=False)\n",
    "\n",
    "    # Plot histogram of the frequency of the mean SHAP value for the instances for each hospital's own patients\n",
    "    # Use from violin plots\n",
    "    # facecolor = [0.12156863, 0.46666667, 0.70588235, 0.3]\n",
    "    # edgecolor= [0.12156863, 0.46666667, 0.70588235, 1.]\n",
    "    fig = plt.figure(figsize=(12,3))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    ax.hist(stroke_team_mean_shap, bins=np.arange(-1.5, 1.5, 0.1), \n",
    "            color=[0.12156863, 0.46666667, 0.70588235, 0.3], \n",
    "            ec=[0.12156863, 0.46666667, 0.70588235, 1.], \n",
    "            linewidth=1.4)\n",
    "    ax.set_title(f\"Outcome for class {class_category}\")\n",
    "    ax.set_xlabel('SHAP values (mean) for hospital attended')\n",
    "    ax.set_ylabel('Count')\n",
    "    plt.savefig(f'./{paths.notebook}{paths.model_text}_hosp_shap_hist.jpg', dpi=300, \n",
    "                bbox_inches='tight', pad_inches=0.2)\n",
    "    plt.show()\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_category in range(n_classes):\n",
    "    create_histogram_data_and_plot(data[\"stroke_team\"],\n",
    "                                   shap_values,\n",
    "                                   features,\n",
    "                                   class_category)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify hospital with zero mean SHAP (to use in the synthetic patient data, notebook 230703_xgb_all_data_synthetic_patients.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_category = 6\n",
    "# set high, as comparison\n",
    "min_mean_shap = 999\n",
    "\n",
    "index = features.index(\"stroke_team\")\n",
    "\n",
    "# Get mean SHAP for stroke team when patient attending that stroke team\n",
    "unique_stroketeams_list = list(np.unique(data[\"stroke_team\"]))\n",
    "# Loop through stroke teams\n",
    "for stroke_team in unique_stroketeams_list:\n",
    "    # Identify rows in test data that match each stroke team\n",
    "    mask = data[\"stroke_team\"] == stroke_team\n",
    "    # Extract those rows, for the class\n",
    "    stroke_team_shap = shap_values[mask,index,class_category]#shap_values[mask]\n",
    "    # Get mean\n",
    "    mean_shap = np.mean(stroke_team_shap)\n",
    "    # Store mean\n",
    "\n",
    "    if abs(mean_shap) < min_mean_shap:\n",
    "        zero_hospital = stroke_team\n",
    "        min_mean_shap = mean_shap\n",
    "\n",
    "# Access the hostpial name from the dictionary, using the value (code) to access the key (hostpial name)\n",
    "zero_hosp_name = [k for k, v in teams_code_dict.items() if v == zero_hospital]\n",
    "print(zero_hosp_name)\n",
    "print(min_mean_shap)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output descriptive text to use in the paper to describe the differences the hospitalfeature value had on the likelihood of receiving thromboylsis. Convert from log odds to odds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "for class_category in range(n_classes):\n",
    "    range_shap_log_odds = max(stroke_team_mean_shap) - min(stroke_team_mean_shap)\n",
    "    odds = math.exp(range_shap_log_odds)\n",
    "    print(f\"There was a {round(odds,2)} fold difference in odds of receiving thrombolysis between hospitals\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot (individual hospitals)\n",
    "Create a boxplot to show the range of SHAP values for each individual one-hot encoded hospital feature.\n",
    "\n",
    "Show the SHAP value as two populations: 1) the group of instances that attend the hospital [black], and 2) the group of instances that do not attend the hosptial [orange].\n",
    "\n",
    "Order the hospitals in descending order of mean SHAP value for the hospital the instance attended (so those that more often contribute to a yes-thrombolysis decision, through to those that most often contribute to a no-thrombolysis decision).\n",
    "\n",
    "Firstly, to order the hospitals, create a dataframe containing the mean SHAP value for each hosptial (for those instances that attended the hospital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get code working for one class\n",
    "c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access hospital names\n",
    "\n",
    "# Get list of one-hot encoded hospital column titles\n",
    "hospital_names_ohe = X_data.filter(regex='^team',axis=1).columns\n",
    "n_hospitals = len(hospital_names_ohe)\n",
    "\n",
    "# Get list of hospital names without the prefix \"team_\"\n",
    "hospital_names = [h[5:] for h in hospital_names_ohe]\n",
    "\n",
    "hospital_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of column indices for these hospital column titles (where do the\n",
    "#   hospital features exist in the datasets?)\n",
    "hospital_columns_index = [X_data.columns.get_loc(col) for col in hospital_names_ohe]\n",
    "# Use this index list to access the hosptial shap values (as array)\n",
    "hosp_shap_values = shap_values_extended[:,:,c].values[:,hospital_columns_index]\n",
    "# Put in dataframe with hospital as column title\n",
    "df_hosp_shap_values = pd.DataFrame(hosp_shap_values, columns = hospital_names)\n",
    "df_hosp_shap_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also include four further columns:\n",
    "\n",
    "the hospital that the instance attended\n",
    "contribution from all of the one-hot encoded hospital features\n",
    "contribution from just the hospital attended\n",
    "contribution from not attending the rest\n",
    "\n",
    "\n",
    "Also store values in shap_not_attend_these_hospitals_individual so can create a histogram of not attended hosptials SHAP values for the first instance (for the class=1 output). Seen in the waterfall plots that individual hospitals not attended can have a large contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include Stroke team that each instance attended\n",
    "df_hosp_shap_values[\"Stroke team\"] = data_stroke_team\n",
    "\n",
    "# Store the sum of the SHAP values (for all of the hospital features)\n",
    "df_hosp_shap_values[\"all_stroke_teams\"] = df_hosp_shap_values.sum(axis=1)\n",
    "\n",
    "# Initialise list for 1) SHAP value for attended hospital 2) SHAP value for \n",
    "#   the sum of the rest of the hospitals\n",
    "shap_attended_hospital = []\n",
    "shap_not_attend_these_hospitals_sum = []\n",
    "# have a list, an element per instance, with each element containing array of the not attended hospital SHAP values (lst created using append)\n",
    "shap_not_attend_these_hospitals_individual_hosp_per_instance = []\n",
    "# have a list, each element containing a not attended hospital SHAP value. The whole dataset as a list of individual values (list created using extend)\n",
    "shap_not_attend_these_hospitals_individual_hosp_values = []\n",
    "\n",
    "# For each patient\n",
    "for index, row in df_hosp_shap_values.iterrows():\n",
    "    \n",
    "    # Get stroke team attended\n",
    "    stroke_team = row[\"Stroke team\"]\n",
    "\n",
    "    # Get stroke teams not attended\n",
    "    set_hospital_names_not_attend = set(hospital_names)\n",
    "    set_hospital_names_not_attend.remove(stroke_team)\n",
    "    \n",
    "    # Get SHAP value for the stroke team attended\n",
    "    shap_attended_hospital.append(row[stroke_team])\n",
    "#    not_attend = row.loc[row.index != stroke_team]\n",
    "    not_attend = row.loc[set_hospital_names_not_attend]\n",
    "    \n",
    "    # Calculate sum of SHAP values for the stroke teams not attend \n",
    "    sum_rest = row[\"all_stroke_teams\"] - row[stroke_team]\n",
    "    shap_not_attend_these_hospitals_sum.append(sum_rest)\n",
    "    shap_not_attend_these_hospitals_individual_hosp_per_instance.append(not_attend.values)\n",
    "    shap_not_attend_these_hospitals_individual_hosp_values.extend(not_attend.values)\n",
    "\n",
    "# Store two new columns in dataframe\n",
    "df_hosp_shap_values[\"attended_stroke_team\"] = shap_attended_hospital\n",
    "df_hosp_shap_values[\"not_attended_stroke_teams\"] = (\n",
    "                                            shap_not_attend_these_hospitals_sum)\n",
    "                   \n",
    "# View preview\n",
    "df_hosp_shap_values.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range of not attended hospital SHAP values for hospitals not attended for the first instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,3))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.hist(shap_not_attend_these_hospitals_individual_hosp_per_instance[0], \n",
    "        color=[0.12156863, 0.46666667, 0.70588235, 0.3], \n",
    "        ec=[0.12156863, 0.46666667, 0.70588235, 1.], \n",
    "        linewidth=1.4)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range of not attended hospital SHAP values for hospitals not attended for all of the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,3))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.hist(shap_not_attend_these_hospitals_individual_hosp_values, \n",
    "        color=[0.12156863, 0.46666667, 0.70588235, 0.3], \n",
    "        ec=[0.12156863, 0.46666667, 0.70588235, 1.], \n",
    "        linewidth=1.4)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot (all hospitals together)\n",
    "Analyse the range of SHAP values for the one-hot encoded hospital features. Show as two populations: 1) the attended hospital, 2) the sum of the hospitals not attended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.boxplot([shap_attended_hospital, shap_not_attend_these_hospitals_sum],\n",
    "            labels=[\"Attended hospital\", \"Not attend these hospitals\"],\n",
    "            whis=99999, notch=True);\n",
    "title = (\"The range of SHAP values for the one-hot encoded hospital features, \"\n",
    "         \"\\ndepending on whether attend the hospital, or not\")\n",
    "\n",
    "# Add line at Shap = 0\n",
    "ax.plot([plt.xlim()[0], plt.xlim()[1]], [0,0], c='0.8') \n",
    "    \n",
    "ax.set_title(title)\n",
    "ax.set_xlabel(\"Instance population\")\n",
    "ax.set_ylabel(\"SHAP value\");\n",
    "\n",
    "#plt.savefig(f'./output/{notebook}_{model_text}'\n",
    "#            f'_hosp_shap_attend_vs_notattend_boxplot.jpg', dpi=300, \n",
    "#            bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise lists\n",
    "attend_stroketeam_min = []\n",
    "attend_stroketeam_q1 = []\n",
    "attend_stroketeam_mean = []\n",
    "attend_stroketeam_q3 = []\n",
    "attend_stroketeam_max = []\n",
    "\n",
    "# For each hospital, store descriptive statistics of SHAP values for those\n",
    "#   instances that attend the hospital\n",
    "for h in hospital_names:\n",
    "    mask = df_hosp_shap_values['Stroke team'] == h\n",
    "    stroke_team_masked_data = df_hosp_shap_values[h][mask]\n",
    "    q1, q3 = np.percentile(stroke_team_masked_data, [25,75])\n",
    "    attend_stroketeam_min.append(stroke_team_masked_data.min())\n",
    "    attend_stroketeam_q1.append(q1)\n",
    "    attend_stroketeam_mean.append(stroke_team_masked_data.mean())\n",
    "    attend_stroketeam_q3.append(q3)\n",
    "    attend_stroketeam_max.append(stroke_team_masked_data.max())\n",
    "    \n",
    "# Create dataframe with 6 columns\n",
    "df_hosp_shap_value_stats = pd.DataFrame(hospital_names, columns=[\"hospital\"])\n",
    "df_hosp_shap_value_stats[\"shap_min\"] = attend_stroketeam_min\n",
    "df_hosp_shap_value_stats[\"shap_q1\"] = attend_stroketeam_q1\n",
    "df_hosp_shap_value_stats[\"shap_mean\"] = attend_stroketeam_mean\n",
    "df_hosp_shap_value_stats[\"shap_q3\"] = attend_stroketeam_q3\n",
    "df_hosp_shap_value_stats[\"shap_max\"] = attend_stroketeam_max\n",
    "\n",
    "# sort in descending mean SHAP value order\n",
    "df_hosp_shap_value_stats.sort_values(\"shap_mean\", ascending=False, inplace=True)                \n",
    "df_hosp_shap_value_stats.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add admission figures to xlabel in boxplot\n",
    "\n",
    "Create dataframe with admissions and thrombolysis rate per stroke team (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Stroke team name, the stroke team admission numbers, and list of SHAP \n",
    "#   values for each instance that attended teh stroke team\n",
    "unique_stroketeams_list = list(set(data_stroke_team))\n",
    "admissions = [X_data[f'team_{s}'].sum() for s in unique_stroketeams_list]\n",
    "\n",
    "df_stroketeam_ivt_adms = pd.DataFrame(unique_stroketeams_list, \n",
    "                                      columns=[\"Stroke team\"])\n",
    "df_stroketeam_ivt_adms[\"Admissions\"] = admissions\n",
    "df_stroketeam_ivt_adms.set_index(\"Stroke team\", inplace=True)\n",
    "df_stroketeam_ivt_adms.sort_values(\"Admissions\", ascending=True, inplace=True)\n",
    "\n",
    "# Calculate IVT rate per hosptial, done at beginning of notebook\n",
    "#hosp_ivt_rate = data.groupby(by=[\"Stroke team\"]).mean()[\"Thrombolysis\"]\n",
    "\n",
    "# Join IVT rate with admissions per hosptial\n",
    "df_stroketeam_ivt_adms = df_stroketeam_ivt_adms.join(hosp_ivt_rate)\n",
    "\n",
    "df_stroketeam_ivt_adms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data for boxplot. Using order of hospitals from the hosp_shap_stats_df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Go through this order of hospitals\n",
    "hospital_order = df_hosp_shap_value_stats[\"hospital\"]\n",
    "    \n",
    "# Create list of SHAP main effect values (one per hospital) for instances that \n",
    "#   attend stroke team\n",
    "attend_stroketeam_groups_ordered = []\n",
    "not_attend_stroketeam_groups_ordered = []\n",
    "\n",
    "# Create list of labels for boxplot \"stroke team name (admissions)\"\n",
    "xlabel = []\n",
    "\n",
    "# Through hospital in defined order (as determined above)\n",
    "for h in hospital_order:\n",
    "    # Attend\n",
    "    mask = df_hosp_shap_values['Stroke team'] == h\n",
    "    attend_stroketeam_groups_ordered.append(df_hosp_shap_values[h][mask])\n",
    "    # Not attend\n",
    "    mask = df_hosp_shap_values['Stroke team'] != h\n",
    "    not_attend_stroketeam_groups_ordered.append(df_hosp_shap_values[h][mask])\n",
    "    # Label\n",
    "    ivt_rate = int(df_stroketeam_ivt_adms['Thrombolysis'].loc[h] * 100)\n",
    "    xlabel.append(f\"{h} ({df_stroketeam_ivt_adms['Admissions'].loc[h]}, \"\n",
    "                  f\"{ivt_rate}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the boxplot\n",
    "\n",
    "Resource for using overall y min and max of both datasets on the 4 plots so have the same range: https://blog.finxter.com/how-to-find-the-minimum-of-a-list-of-lists-in-python/#:~:text=With%20the%20key%20argument%20of,of%20the%20list%20of%20lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 30 hospitals on each subplot to aid readability\n",
    "print(\"Shows the range of contributions to the prediction from this hospital \"\n",
    "      \"when patients 1) do [black], and 2) do not [orange] attend this \"\n",
    "      \"hospital\")\n",
    "\n",
    "# Group the hospitals into 30\n",
    "st = 0\n",
    "ed = 30\n",
    "inc = ed\n",
    "max_size = n_hospitals\n",
    "\n",
    "# Use overall y min & max of both datasets on the 4 plots so have same range\n",
    "ymin1 = min(min(attend_stroketeam_groups_ordered, key=min))\n",
    "ymin2 = min(min(not_attend_stroketeam_groups_ordered, key=min))\n",
    "ymax1 = max(max(attend_stroketeam_groups_ordered, key=max))\n",
    "ymax2 = max(max(not_attend_stroketeam_groups_ordered, key=max))\n",
    "ymin = min(ymin1, ymin2)\n",
    "ymax = max(ymax1, ymax2)\n",
    "\n",
    "# Adjust min and max to accommodate some wriggle room\n",
    "yrange = ymax - ymin1\n",
    "ymin = ymin - yrange/50\n",
    "ymax = ymax + yrange/50\n",
    "\n",
    "# Create figure with 4 subplots\n",
    "fig = plt.figure(figsize=(12,35))\n",
    "\n",
    "# Create four subplots\n",
    "for subplot in range(4):\n",
    "    ax = fig.add_subplot(4,1,subplot+1)\n",
    "\n",
    "    # The contribution from this hospital when patients don't attend this \n",
    "    #    hospital\n",
    "    c1 = \"orange\"\n",
    "    c2 = \"orange\"\n",
    "    c3 = \"white\"\n",
    "    ax.boxplot(not_attend_stroketeam_groups_ordered[st:ed],\n",
    "               labels=xlabel[st:ed],\n",
    "               whis=99999, patch_artist=True, notch=True,\n",
    "               boxprops=dict(facecolor=c3, color=c1),\n",
    "               capprops=dict(color=c1),\n",
    "               whiskerprops=dict(color=c1),\n",
    "               flierprops=dict(color=c1, markeredgecolor=c1),\n",
    "               meanprops=dict(color=c2))\n",
    "       \n",
    "    # The contribution from this hospital when patients do attend this hosptial\n",
    "    c1 = \"black\"\n",
    "    c2 = \"black\"\n",
    "    c3 = \"white\"\n",
    "    box_plot = ax.boxplot(attend_stroketeam_groups_ordered[st:ed],\n",
    "               labels=xlabel[st:ed],\n",
    "               whis=99999, patch_artist=True, notch=True,\n",
    "               boxprops=dict(facecolor=c3, color=c1),\n",
    "               capprops=dict(color=c1),\n",
    "               whiskerprops=dict(color=c1),\n",
    "               flierprops=dict(color=c1, markeredgecolor=c1),\n",
    "               meanprops=dict(color=c2))\n",
    "\n",
    "    # Set the median for attended hospital boxplots to grey (otherwise default \n",
    "    # was orange - confusing when not attended hospital colour choice above is orange)\n",
    "    for median in box_plot['medians']:\n",
    "        median.set_color('grey')\n",
    "\n",
    "    plt.ylabel('SHAP values (one-hot encoded hospital feature)',size=12)\n",
    "    plt.xlabel('Stroke team (admissions, IVT rate)',size=12)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.xticks(rotation=90)\n",
    "    # Add line at Shap = 0\n",
    "    plt.plot([plt.xlim()[0], plt.xlim()[1]], [0,0], c='0.8') \n",
    "    st = min(st+inc,max_size)\n",
    "    ed = min(ed+inc,max_size)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.25, wspace=0.05)\n",
    "        shap_per_category = []\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "\n",
    "#plt.savefig(f'./output/{notebook}_{model_text}'\n",
    "#            f'_individual_hosp_shap_attend_vs_notattend_boxplot.jpg', dpi=300, \n",
    "#            bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "f85b883bff9a8a9f39576b94acbdf6672b3dc17c35647e7395f81e785740a4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
