{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost full model\n",
    "(using stratified 5-fold cross validation)\n",
    "\n",
    "### Plain English summary\n",
    "\n",
    "\n",
    "### Model and data\n",
    "\n",
    "### Aims\n",
    "\n",
    "### Observations\n",
    "\n",
    "Using code from https://github.com/samuel-book/samuel_shap_paper_1/blob/main/xgb_10_features/02b_xgb_combined_fit_accuracy_key_features_separate_k_fold.ipynb\n",
    "and https://github.com/samuel-book/samuel_shap_paper_1/blob/main/xgb_10_features/03_xgb_combined_shap_key_features.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "# Import local package\n",
    "from utils import waterfall\n",
    "# Force package to be reloaded\n",
    "importlib.reload(waterfall);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    '''Singleton object for storing paths to data and database.'''\n",
    "\n",
    "  #  data_path: str = '../'\n",
    "  #  data_filename: str = 'SAMueL ssnap extract v2.csv'\n",
    "    data_save_path: str = './'\n",
    "  #  data_save_filename: str = 'reformatted_data.csv'\n",
    "  #  database_filename: str = 'samuel.db'\n",
    "  #  notebook: str = '01'\n",
    "  #  kfold_folder: str = 'data/kfold_5fold/'\n",
    "\n",
    "    data_read_path: str = '../data/kfold_5fold'\n",
    "#    data_read_filename: str = '03_reformatted_data_ml.csv'\n",
    " #   data_save_path: str = './kfold_5fold'\n",
    "#    data_save_filename: str = 'train.csv'\n",
    "    model_text: str = 'xgb_full_model'\n",
    "    notebook: str = '03_'\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Data has previously been split into 5 stratified k-fold splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = [], []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    train_data.append(pd.read_csv(paths.data_read_path + '/03_train_{0}.csv'.format(i)))\n",
    "    test_data.append(pd.read_csv(paths.data_read_path + '/03_test_{0}.csv'.format(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 51 features\n"
     ]
    }
   ],
   "source": [
    "features = list(train_data[0])\n",
    "print(f\"There are {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot the categorical features\n",
    "\n",
    "Convert some categorical features to one hot encoded features.\n",
    "\n",
    "Define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_to_one_hot(df, feature_name, prefix):\n",
    "    \"\"\"\n",
    "    df [dataframe]: training or test dataset\n",
    "    feature_name [str]: feature to convert to ont hot encoding\n",
    "    prefix [str]: string to use on new feature\n",
    "    \"\"\"\n",
    "\n",
    "    # One hot encode a feature\n",
    "    df_feature = pd.get_dummies(\n",
    "        df[feature_name], prefix = prefix)\n",
    "    df = pd.concat([df, df_feature], axis=1)\n",
    "    df.drop(feature_name, axis=1, inplace=True)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up two lists for the one hot encoding. \n",
    "\n",
    "A list of the feature names that are categorical and to be converted using one hot encoding.\n",
    "A list of the prefixes to use for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_one_hot = [\"stroke_team\", \"weekday\"]\n",
    "list_prefix = [\"team\", \"weekday\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature in the list, for each train and test dataset, convert to one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, prefix in zip(features_to_one_hot, list_prefix):\n",
    "    for k_fold in range(5):\n",
    "        train_data[k_fold] = convert_feature_to_one_hot(train_data[k_fold], feature, prefix)\n",
    "        test_data[k_fold] = convert_feature_to_one_hot(test_data[k_fold], feature, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each train and test set not necessarily have all columns.\n",
    "Check if each train and test dataset contains each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_one_hot_features = set()\n",
    "for k_fold in range(5):\n",
    "    temp_set = set(train_data[k_fold].columns).union(set(test_data[k_fold].columns))\n",
    "    set_one_hot_features = set_one_hot_features.union(temp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print if any columns are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_fold in range(5):\n",
    "    if len(list(set_one_hot_features - set(train_data[k_fold].columns))) > 0:\n",
    "        print(f\"Train kfold {k_fold}: {set_one_hot_features - set(train_data[k_fold].columns)}\")\n",
    "    if len(list(set_one_hot_features - set(test_data[k_fold].columns))) > 0:\n",
    "        print(f\"Test kfold {k_fold}: {set_one_hot_features - set(test_data[k_fold].columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit XGBoost model\n",
    "\n",
    "Train model with all features (same as used in notebook 02_xgb_feature_selection.ipynb for comparison with the feature selection models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_square_kfold = []\n",
    "roc_auc_ovr_kfold = []\n",
    "accuracy_kfold = []\n",
    "error_within_one_kfold = []\n",
    "feature_importance_kfold = []\n",
    "y_probs_kfold = []\n",
    "y_pred_kfold = []\n",
    "model_kfold = []\n",
    "\n",
    "# Loop through k folds\n",
    "for k_fold in range(5):\n",
    "\n",
    "    # Get k fold split\n",
    "    train = train_data[k_fold]\n",
    "    test = test_data[k_fold]\n",
    "\n",
    "    # Get X and y\n",
    "    X_train = train.drop('discharge_disability', axis=1)\n",
    "    X_test = test.drop('discharge_disability', axis=1)\n",
    "    y_train = train['discharge_disability']\n",
    "    y_test = test['discharge_disability']\n",
    "\n",
    "    # One hot encode categorical features\n",
    "    for feature, prefix in zip(features_to_one_hot, list_prefix):\n",
    "        X_train = convert_feature_to_one_hot(X_train, feature, prefix)\n",
    "        X_test = convert_feature_to_one_hot(X_test, feature, prefix)\n",
    "\n",
    "    # Define model\n",
    "    model = XGBClassifier(verbosity = 0, seed=42, learning_rate=0.5)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    model_kfold.append(model)\n",
    "    \n",
    "    # Get target categories from model\n",
    "    classes = model.classes_\n",
    "\n",
    "    # Get and store predicted probabilities\n",
    "    y_probs = model.predict_proba(X_test)\n",
    "    y_probs_kfold.append(y_probs)\n",
    "\n",
    "    # Get and store predicted class\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_kfold.append(y_pred)\n",
    "\n",
    "    # Calculate ROC AUC for multiclass models, using One vs Rest\n",
    "    roc_auc_ovr = roc_auc_score(y_test, y_probs, labels = classes, \n",
    "                                multi_class = 'ovr', average = 'macro')\n",
    "    roc_auc_ovr_kfold.append(roc_auc_ovr)\n",
    "\n",
    "    accuracy = np.mean(y_error==0)\n",
    "    accuracy_kfold.append(f'Accuracy: {accuracy:0.2f}')\n",
    "\n",
    "    error_within_one = np.mean(np.abs(y_error)<=1)\n",
    "    error_within_one_kfold.append(f'Error within 1: {error_within_one:0.2f}')\n",
    "\n",
    "    # Get and store feature importances\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance_kfold.append(feature_importance)\n",
    "\n",
    "# Get average result from all k-fold splits\n",
    "roc_auc_ovr_mean = np.mean(roc_auc_ovr_kfold)\n",
    "accuracy_mean = np.mean(accuracy_kfold)\n",
    "error_within_one_mean = np.mean(error_within_one_kfold)\n",
    "\n",
    "print (f'All features, AUC: {roc_auc_ovr_mean:0.3f} '\n",
    "       f'(std across 5 kfolds: {np.std(roc_auc_ovr_kfold):0.3f})')\n",
    "\n",
    "print (f'All features, accuracy: {accuracy_mean:0.3f} '\n",
    "       f'(std across 5 kfolds: {np.std(accuracy_kfold):0.3f})')\n",
    "\n",
    "print (f'All features, accuracy within one: {error_within_one_mean:0.3f} '\n",
    "       f'(std across 5 kfolds: {np.std(error_within_one_kfold):0.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "Get XGBoost feature importances (average across k-fold results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names\n",
    "feature_names = X_test_kfold[0].columns.values\n",
    "\n",
    "# Get average feature importance from k-fold\n",
    "importances = np.array(feature_importance_kfold).mean(axis = 0)\n",
    "\n",
    "# Store in DataFrame\n",
    "feature_importance_df = pd.DataFrame(data = importances, index=feature_names)\n",
    "feature_importance_df.columns = ['importance']\n",
    "\n",
    "# Sort by importance (weight)\n",
    "feature_importance_df.sort_values(by='importance', \n",
    "                                  ascending=False, inplace=True)\n",
    "\n",
    "# Save\n",
    "#feature_importance_df.to_csv(f'output/{notebook}_{model_type}_feature_importance.csv')\n",
    "\n",
    "# Display top 25\n",
    "feature_importance_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a bar chart for the XGBoost feature importance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Get labels and values\n",
    "labels = feature_importance_df.index.values[0:25]\n",
    "pos = np.arange(len(labels))\n",
    "val = feature_importance_df['importance'].values[0:25]\n",
    "\n",
    "# Plot\n",
    "ax.bar(pos, val)\n",
    "ax.set_ylabel('Feature importance')\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f'output/{notebook}_{model_type}_feature_weights_bar.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP values\n",
    "SHAP values give the contribution that each feature has on the models prediction, per instance. A SHAP value is returned for each feature, for each instance.\n",
    "\n",
    "We will use the shap library: https://shap.readthedocs.io/en/latest/index.html\n",
    "\n",
    "'Raw' SHAP values from XGBoost model are log odds ratios. A SHAP value is returned for each feature, for each instance, for each model (one per k-fold)\n",
    "\n",
    "## Get SHAP values\n",
    "TreeExplainer is a fast and exact method to estimate SHAP values for tree models and ensembles of trees. Using this we can calculate the SHAP values.\n",
    "\n",
    "Either load from pickle (if file exists), or calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise empty lists\n",
    "shap_values_extended_kfold = []\n",
    "shap_values_kfold = []\n",
    "\n",
    "# For each k-fold split\n",
    "for k in range(5):\n",
    "    # Set filename\n",
    "    filename = (f'{paths.data_save_path}{paths.notebook}_{paths.model_text}_shap_values_extended_{k}.p')\n",
    "    # Check if exists\n",
    "    file_exists = exists(filename)\n",
    "    \n",
    "    if file_exists:\n",
    "        # Load explainer\n",
    "        with open(filename, 'rb') as filehandler:\n",
    "            shap_values_extended = pickle.load(filehandler)\n",
    "            shap_values_extended_kfold.append(shap_values_extended)\n",
    "            shap_values_kfold.append(shap_values_extended.values)\n",
    "    else:\n",
    "        # Calculate SHAP values\n",
    "        \n",
    "        # Set up explainer using the model and feature values from training set\n",
    "        explainer = shap.TreeExplainer(model_kfold[k], X_train_kfold[k])\n",
    "\n",
    "        # Get (and store) Shapley values along with base and feature values\n",
    "        shap_values_extended = explainer(X_test_kfold[k])\n",
    "        shap_values_extended_kfold.append(shap_values_extended)\n",
    "        # Shap values exist for each classification in a Tree\n",
    "        # We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "        shap_values = shap_values_extended.values\n",
    "        shap_values_kfold.append(shap_values)        \n",
    "\n",
    "        explainer_filename = (f'{paths.data_save_path}{paths.notebook}_{paths.model_text}_shap_explainer_{k}.p')\n",
    "\n",
    "        # Save explainer using pickle\n",
    "        with open(explainer_filename, 'wb') as filehandler:\n",
    "            pickle.dump(explainer, filehandler)\n",
    "            \n",
    "        # Save shap values extendedr using pickle\n",
    "        with open(filename, 'wb') as filehandler:\n",
    "            pickle.dump(shap_values_extended, filehandler)\n",
    "        \n",
    "        # Print progress\n",
    "        print (f'Completed {k+1} of 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration and assessment of accuracy when model has high confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate results in Dataframe\n",
    "reliability_collated = pd.DataFrame()\n",
    "\n",
    "# Loop through k fold predictions\n",
    "for k in range(5):\n",
    "    \n",
    "    # Get observed class and predicted probability\n",
    "    observed = y_test_kfold[k]\n",
    "    probability = y_pred_proba_kfold[k]\n",
    "    \n",
    "    # Bin data with numpy digitize (this will assign a bin to each case)\n",
    "    step = 0.10\n",
    "    bins = np.arange(step, 1+step, step)\n",
    "    digitized = np.digitize(probability, bins)\n",
    "        \n",
    "    # Put single fold data in DataFrame\n",
    "    reliability = pd.DataFrame()\n",
    "    reliability['bin'] = digitized\n",
    "    reliability['probability'] = probability\n",
    "    reliability['observed'] = observed\n",
    "    classification = 1 * (probability > 0.5 )\n",
    "    reliability['correct'] = observed == classification\n",
    "    reliability['count'] = 1\n",
    "    \n",
    "    # Summarise data by bin in new dataframe\n",
    "    reliability_summary = pd.DataFrame()\n",
    "\n",
    "    # Add bins and k-fold to summary\n",
    "    reliability_summary['bin'] = bins\n",
    "    reliability_summary['k-fold'] = k\n",
    "\n",
    "    # Calculate mean of predicted probability of thrombolysis in each bin\n",
    "    reliability_summary['confidence'] = \\\n",
    "        reliability.groupby('bin').mean()['probability']\n",
    "\n",
    "    # Calculate the proportion of patients who receive thrombolysis\n",
    "    reliability_summary['fraction_positive'] = \\\n",
    "        reliability.groupby('bin').mean()['observed']\n",
    "    \n",
    "    # Calculate proportion correct in each bin\n",
    "    reliability_summary['fraction_correct'] = \\\n",
    "        reliability.groupby('bin').mean()['correct']\n",
    "    \n",
    "    # Calculate fraction of results in each bin\n",
    "    reliability_summary['fraction_results'] = \\\n",
    "        reliability.groupby('bin').sum()['count'] / reliability.shape[0]   \n",
    "    \n",
    "    # Add k-fold results to DatafRame collation\n",
    "    reliability_collated = reliability_collated.append(reliability_summary)\n",
    "    \n",
    "# Get mean results\n",
    "reliability_summary = reliability_collated.groupby('bin').mean()\n",
    "reliability_summary.drop('k-fold', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get average SHAP values for each k-fold\n",
    "For each k-fold split, calculate the mean SHAP value for each feature (across all instances). The mean is calculated in three ways:\n",
    "\n",
    "mean of raw values\n",
    "mean of absolute values\n",
    "absolute of mean of raw values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise empty lists\n",
    "shap_values_mean_kfold = []\n",
    "\n",
    "# For each k-fold split\n",
    "for k in range(5):\n",
    "    # Calculate mean SHAP value for each feature (across all instances)\n",
    "    shap_values = shap_values_kfold[k]\n",
    "    df = pd.DataFrame(index=feature_names_ohe)\n",
    "    df['mean_shap'] = np.mean(shap_values, axis=0)\n",
    "    df['abs_mean_shap'] = np.abs(df['mean_shap'])\n",
    "    df['mean_abs_shap'] = np.mean(np.abs(shap_values), axis=0)\n",
    "    df['rank'] = df['mean_abs_shap'].rank(\n",
    "        ascending=False).values\n",
    "    df.sort_index()\n",
    "    shap_values_mean_kfold.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine consistency of SHAP values across k-fold splits\n",
    "A model is fitted to each k-fold split, and SHAP values are obtained for each model. This next section assesses the range of SHAP values (mean |SHAP|) for each feature across the k-fold splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise DataFrame (stores mean of the absolute SHAP values for each kfold)\n",
    "df_mean_abs_shap = pd.DataFrame()\n",
    "\n",
    "# For each k-fold split\n",
    "for k in range(5):\n",
    "    # mean of the absolute SHAP values for each k-fold split\n",
    "    df_mean_abs_shap[f'{k}'] = shap_values_mean_kfold[k]['mean_abs_shap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_abs_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create (and show) a dataframe that stores the min, median, and max SHAP values for each feature across the 5 k-fold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_abs_shap_summary = pd.DataFrame()\n",
    "df_mean_abs_shap_summary['min'] = df_mean_abs_shap.min(axis=1)\n",
    "df_mean_abs_shap_summary['median'] = df_mean_abs_shap.median(axis=1) \n",
    "df_mean_abs_shap_summary['max'] = df_mean_abs_shap.max(axis=1)\n",
    "df_mean_abs_shap_summary.sort_values('median', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_mean_abs_shap_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the 10 features with the highest SHAP values (in terms of the median of the kfolds of the mean of the absolute SHAP values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_shap = list(df_mean_abs_shap_summary.head(10).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a violin plot for these 10 features with the highest SHAP values.\n",
    "\n",
    "A violin plot shows the distribution of the SHAP values for each feature across the 5 kfold splits. The bars show the min, median and max values for the feature, and the shaded area around the bars show the density of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.violinplot(df_mean_abs_shap.loc[top_10_shap].T,\n",
    "               showmedians=True,\n",
    "               widths=1)\n",
    "ax1.set_ylim(0)\n",
    "labels = top_10_shap\n",
    "ax1.set_xticks(np.arange(1, len(labels) + 1))\n",
    "ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax1.grid(which='both')\n",
    "ax1.set_ylabel('|SHAP value| (log odds)')\n",
    "\n",
    "\n",
    "filename = (f'{paths.data_save_path}{paths.notebook}_{paths.model_text}_shap_violin.jpg')\n",
    "\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the consitency of feature importances across k-fold splits\n",
    "XGBoost algorithm provides a metrc per feature called \"feature importance\".\n",
    "\n",
    "A model is fitted to each k-fold split, and feature importance values are obtained for each model. This next section assesses the range of feature importance values for each feature across the k-fold splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialise DataFrame (stores feature importance values for each kfold)\n",
    "df_feature_importance = pd.DataFrame()\n",
    "\n",
    "# For each k-fold\n",
    "for k in range(5):\n",
    "    # feature importance value for each k-fold split\n",
    "    df_feature_importance[f'{k}'] = feature_importance_kfold[k]['importance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create (and show) a dataframe that stores the min, median, and max feature importance value for each feature across the 5 k-fold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_summary = pd.DataFrame()\n",
    "df_feature_importance_summary['min'] = df_feature_importance.min(axis=1)\n",
    "df_feature_importance_summary['median'] = df_feature_importance.median(axis=1) \n",
    "df_feature_importance_summary['max'] = df_feature_importance.max(axis=1)\n",
    "df_feature_importance_summary.sort_values('median', inplace=True, \n",
    "                                          ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the 10 features with the highest feature importance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_importances = list(df_feature_importance_summary.head(10).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a violin plot for these 10 features with the highest feature importance values.\n",
    "\n",
    "A violin plot shows the distribution of the feature importance values for each feature across the 5 kfold splits. The bars show the min, median and max values for the feature, and the shaded area around the bars show the density of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.violinplot(df_feature_importance_summary.loc[top_10_importances].T,\n",
    "              showmedians=True,\n",
    "              widths=1)\n",
    "ax1.set_ylim(0)\n",
    "labels = top_10_importances\n",
    "ax1.set_xticks(np.arange(1, len(labels) + 1))\n",
    "ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax1.grid(which='both')\n",
    "ax1.set_ylabel('Importance')\n",
    "\n",
    "filename = (f'{paths.data_save_path}{paths.notebook}_{paths.model_text}_importance_violin.jpg')\n",
    "\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare top 10 SHAP and feature importance values\n",
    "Compare the features (and their values) that make the top 10 when selected by either SHAP values, or feature importance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_shap_importance = pd.DataFrame()\n",
    "df_compare_shap_importance['SHAP (feature name)'] = \\\n",
    "                            df_mean_abs_shap_summary.index\n",
    "df_compare_shap_importance['SHAP (median value)']  = \\\n",
    "                            df_mean_abs_shap_summary['median'].values\n",
    "df_compare_shap_importance['Importance (feature name)'] = \\\n",
    "                            df_feature_importance_summary.index\n",
    "df_compare_shap_importance['Importance (median value)'] = \\\n",
    "                            df_feature_importance_summary['median'].values\n",
    "df_compare_shap_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all of the features, showing feature importance vs SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap_importance = pd.DataFrame()\n",
    "df_shap_importance['Shap'] = df_mean_abs_shap_summary['median']\n",
    "df_shap_importance = df_shap_importance.merge(\n",
    "    df_feature_importance_summary['median'], left_index=True, right_index=True)\n",
    "df_shap_importance.rename(columns={'median':'Importance'}, inplace=True)\n",
    "df_shap_importance.sort_values('Shap', inplace=True, ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(df_shap_importance['Shap'],\n",
    "            df_shap_importance['Importance'])\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlabel('SHAP value (median of the k-folds [mean |Shap|])')\n",
    "ax1.set_ylabel('Importance values (median of the k-folds)')\n",
    "ax1.grid()\n",
    "\n",
    "filename = (f'{paths.data_save_path}{paths.notebook}_{paths.model_text}_shap_importance_correlation.jpg')\n",
    "plt.savefig(filename, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# Plot predicted prob vs fraction psotive\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "\n",
    "# Loop through k-fold reliability results\n",
    "for k in range(5):\n",
    "    mask = reliability_collated['k-fold'] == k\n",
    "    result_kfold = reliability_collated[mask]\n",
    "    x = result_kfold['confidence']\n",
    "    y = result_kfold['fraction_positive']\n",
    "    ax1.plot(x,y, color='orange')\n",
    "# Add 1:1 line\n",
    "ax1.plot([0,1],[0,1], color='k', linestyle ='--')\n",
    "# Refine plot\n",
    "ax1.set_xlabel('Model probability')\n",
    "ax1.set_ylabel('Fraction positive')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot accuracy vs probability\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "# Loop through k-fold reliability results\n",
    "for k in range(5):\n",
    "    mask = reliability_collated['k-fold'] == k\n",
    "    result_kfold = reliability_collated[mask]\n",
    "    x = result_kfold['confidence']\n",
    "    y = result_kfold['fraction_correct']\n",
    "    ax2.plot(x,y, color='orange')\n",
    "# Refine plot\n",
    "ax2.set_xlabel('Model probability')\n",
    "ax2.set_ylabel('Fraction correct')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# instantiate a second axes that shares the same x-axis\n",
    "ax3 = ax2.twinx()  \n",
    "for k in range(5):\n",
    "    mask = reliability_collated['k-fold'] == k\n",
    "    result_kfold = reliability_collated[mask]\n",
    "    x = result_kfold['confidence']\n",
    "    y = result_kfold['fraction_results']\n",
    "    ax3.plot(x,y, color='blue')\n",
    "    \n",
    "ax3.set_xlim(0, 1)\n",
    "ax3.set_ylim(0, 0.5)\n",
    "ax3.set_ylabel('Fraction of samples')\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color='orange', alpha=0.6, lw=2),\n",
    "                Line2D([0], [0], color='blue', alpha = 0.6,lw=2)]\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "\n",
    "plt.legend(custom_lines, ['Fraction correct', 'Fraction of samples'],\n",
    "          loc='upper center')\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "\n",
    "#plt.savefig(f'./output/{notebook}_{model_type}_reliability.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy of model when model is at least 80% confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0.1, 0.2, 0.9, 1.0]\n",
    "acc = reliability_summary.loc[bins].mean()['fraction_correct']\n",
    "frac = reliability_summary.loc[bins].sum()['fraction_results']\n",
    "\n",
    "print ('For samples with at least 80% confidence:')\n",
    "print (f'Proportion of all samples: {frac:0.3f}')\n",
    "print (f'Accuracy: {acc:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('sam10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f85b883bff9a8a9f39576b94acbdf6672b3dc17c35647e7395f81e785740a4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
