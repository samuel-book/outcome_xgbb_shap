{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Training model to predict whether patient dies, or not.\n",
    "\n",
    "1) binary model (dead, not dead)\n",
    "2) multi-class classification model and using just the mRS6 classification outcome"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import importlib\n",
    "# Import local package\n",
    "from utils import waterfall\n",
    "# Force package to be reloaded\n",
    "importlib.reload(waterfall);\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the time duration to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    '''Singleton object for storing paths to data and database.'''\n",
    "\n",
    "  #  data_path: str = '../'\n",
    "  #  data_filename: str = 'SAMueL ssnap extract v2.csv'\n",
    "  #  data_save_path: str = './'\n",
    "  #  data_save_filename: str = 'reformatted_data.csv'\n",
    "  #  database_filename: str = 'samuel.db'\n",
    "  #  notebook: str = '01'\n",
    "  #  kfold_folder: str = 'data/kfold_5fold/'\n",
    "\n",
    "    data_read_path: str = '../data/'\n",
    "    data_read_filename: str = '02_reformatted_data_ml_230612.csv'\n",
    " #   data_save_path: str = './kfold_5fold'\n",
    "#    data_save_filename: str = 'train.csv'\n",
    "    notebook: str = '230725_'\n",
    "    model_text: str = 'xgb_all_data_binary'\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Data has previously been split into 5 stratified k-fold splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths.data_read_path + paths.data_read_filename\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = data['discharge_disability'].unique()\n",
    "class_names = np.sort(class_names)\n",
    "n_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to use onset to thrombolysis time in the model. Define function to calculate the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_onset_to_thrombolysis(row):\n",
    "    # Set default value of onset to thrombolysis of -100 (no thrombolysis given)\n",
    "    onset_to_thrombolysis = -100\n",
    "    # Set value if thrombolysis given\n",
    "    if  row['scan_to_thrombolysis_time'] != -100:\n",
    "        onset_to_thrombolysis = (row['onset_to_arrival_time'] + \n",
    "        row['arrival_to_scan_time'] + row['scan_to_thrombolysis_time'])\n",
    "    return onset_to_thrombolysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate onset to thgrombolysis (but set to -100 if no thrombolysis given)\n",
    "data['onset_to_thrombolysis_time'] = data.apply(calculate_onset_to_thrombolysis, axis=1)\n",
    "data.drop(['scan_to_thrombolysis_time', 'arrival_to_scan_time',\n",
    "        'onset_to_arrival_time'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(data)\n",
    "print(f\"There are {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the binary outcome: Dead or not dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"discharge_mrs6\"] = 0\n",
    "mask = data[\"discharge_disability\"] == 6\n",
    "data[\"discharge_mrs6\"][mask] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['prior_disability','stroke_severity','stroke_team',\n",
    "                     'onset_to_thrombolysis_time','age']\n",
    "X_data = data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_mc = data['discharge_disability']\n",
    "y_data_bin = data['discharge_mrs6']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot the categorical features\n",
    "\n",
    "Convert some categorical features to one hot encoded features.\n",
    "\n",
    "Define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_to_one_hot(df, feature_name, prefix):\n",
    "    \"\"\"\n",
    "    df [dataframe]: training or test dataset\n",
    "    feature_name [str]: feature to convert to ont hot encoding\n",
    "    prefix [str]: string to use on new feature\n",
    "    \"\"\"\n",
    "\n",
    "    # One hot encode a feature\n",
    "    df_feature = pd.get_dummies(\n",
    "        df[feature_name], prefix = prefix)\n",
    "    df = pd.concat([df, df_feature], axis=1)\n",
    "    df.drop(feature_name, axis=1, inplace=True)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up two lists for the one hot encoding. \n",
    "\n",
    "A list of the feature names that are categorical and to be converted using one hot encoding.\n",
    "A list of the prefixes to use for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_one_hot = [\"stroke_team\"]\n",
    "list_prefix = [\"team\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, prefix in zip(features_to_one_hot, list_prefix):\n",
    "    X_data = convert_feature_to_one_hot(X_data, feature, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of features in dataset, post one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ohe = list(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "\n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    false_negatives = (predicted_negatives == 1) & (observed_negatives == 0)\n",
    "       \n",
    "    sensitivity = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['accuracy'] = np.mean(predicted == observed)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Predict binary death (use defaulst threshold, 0.5)\n",
    "\n",
    "Fit XGBoost model, train model with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{paths.notebook}{paths.model_text}_bin.p\"\n",
    "\n",
    "# Check if exists\n",
    "file_exists = exists(filename)\n",
    "\n",
    "if file_exists:\n",
    "    # load model\n",
    "    with open(filename, 'rb') as filehandler:\n",
    "        model = pickle.load(filehandler)\n",
    "else:        \n",
    "\n",
    "    # Define model\n",
    "    model_bin = XGBClassifier(verbosity = 0, seed=42, learning_rate=0.5)\n",
    "\n",
    "    # Fit model\n",
    "    model_bin.fit(X_data, y_data_bin)\n",
    "\n",
    "    # Save model\n",
    "    with open(filename, 'wb') as filehandler:\n",
    "        pickle.dump(model_bin, filehandler)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs_bin = model_bin.predict_proba(X_data)\n",
    "y_pred_bin = model_bin.predict(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show accuracy (identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(y_data_bin, y_pred_bin)\n",
    "print (f'Accuracy (binary model): {accuracy[\"accuracy\"]:0.2f}')\n",
    "print (f'Sensitivity (binary model): {accuracy[\"sensitivity\"]:0.2f}')\n",
    "print (f'Specificity (binary model): {accuracy[\"specificity\"]:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given so few die, a model classifying all as alive gives this accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_bin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_bin.value_counts()[0]/y_data_bin.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_data_bin, y_pred_bin)\n",
    "\n",
    "# Visualize the confusion matrix with colors\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "heatmap = plt.imshow(cm, cmap=plt.cm.Blues, origin='lower')\n",
    "colorbar = plt.colorbar(heatmap, shrink=0.8, ax=ax1, alpha=0.5, label='Count')\n",
    "\n",
    "# To add values to plot\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        plt.text(j, i, cm[i][j], ha='center', va='center', color='r')\n",
    "\n",
    "ax1.set_xlabel('Predicted value')\n",
    "ax1.set_ylabel('Observed value')\n",
    "ax1.set_title('Binary model (default threshold, 0.5)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operator Characteristic and Sensitivity-Specificity Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic Curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_data_bin, y_probs_bin[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Show area under curve  \n",
    "print (f'\\nAUC: {roc_auc:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate data for sensitivity-specificity curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = []\n",
    "specificity = []\n",
    "\n",
    "# Get classification probabilities for k-fold replicate\n",
    "observed = y_data_bin\n",
    "proba = y_probs_bin[:,1]\n",
    "\n",
    "# Loop through increments in probability of survival\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "for cutoff in thresholds: #  loop 0 --> 1 on steps of 0.1\n",
    "    # Get classificiation using cutoff\n",
    "    predicted_class = proba >= cutoff\n",
    "    predicted_class = predicted_class * 1.0\n",
    "    # Call accuracy measures function\n",
    "    accuracy = calculate_accuracy(observed, predicted_class)\n",
    "    # Add accuracy scores to lists\n",
    "    sensitivity.append(accuracy['sensitivity'])\n",
    "    specificity.append(accuracy['specificity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a combined plot: ROC and sensitivity-specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# Plot ROC\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(fpr, tpr, color='orange')\n",
    "ax1.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Receiver Operator Characteristic Curve')\n",
    "text = f'AUC: {roc_auc:.3f}'\n",
    "ax1.text(0.64,0.07, text, \n",
    "         bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot sensitivity-specificity\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(sensitivity, specificity)\n",
    "ax2.set_xlabel('Sensitivity')\n",
    "ax2.set_ylabel('Specificity')\n",
    "ax2.set_title('Sensitivity-Specificity Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "filename = f\"{paths.notebook}{paths.model_text}_bin_roc_sens_spec.jpg\"\n",
    "plt.savefig(filename, dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify cross-over point on sensitivity-specificity curve\n",
    "Adjusting the classification threshold allows us to balance sensitivity (the proportion of patients receiving thrombolysis correctly identified) and specificity (the proportion of patients not receiving thrombolysis correctly identified). An increase in sensitivity causes a loss in specificity (and vice versa). Here we identify the pint where specificity and sensitivity hold the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect(a1, a2, b1, b2):\n",
    "    \"\"\" \n",
    "    Returns the point of intersection of the lines passing through a2,a1 and \n",
    "    b2,b1.\n",
    "    \n",
    "    a1: [x, y] a point on the first line\n",
    "    a2: [x, y] another point on the first line\n",
    "    b1: [x, y] a point on the second line\n",
    "    b2: [x, y] another point on the second line\n",
    "    \"\"\"\n",
    "    s = np.vstack([a1,a2,b1,b2])        # s for stacked\n",
    "    h = np.hstack((s, np.ones((4, 1)))) # h for homogeneous\n",
    "    l1 = np.cross(h[0], h[1])           # get first line\n",
    "    l2 = np.cross(h[2], h[3])           # get second line\n",
    "    x, y, z = np.cross(l1, l2)          # point of intersection\n",
    "    if z == 0:                          # lines are parallel\n",
    "        return (float('inf'), float('inf'))\n",
    "    return (x/z, y/z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['sensitivity'] = np.array(sensitivity)\n",
    "df['specificity'] = np.array(specificity)\n",
    "df['spec greater sens'] = df['specificity'] > df['sensitivity']\n",
    "\n",
    "# find last index for senitivity being greater than specificity \n",
    "mask = df['spec greater sens'] == False\n",
    "last_id_sens_greater_spec = np.max(df[mask].index)\n",
    "locs = [last_id_sens_greater_spec, last_id_sens_greater_spec + 1]\n",
    "points = df.iloc[locs][['sensitivity', 'specificity']]\n",
    "\n",
    "# Get intersetction with line of x=y\n",
    "a1 = list(points.iloc[0].values)\n",
    "a2 = list(points.iloc[1].values)\n",
    "b1 = [0, 0]\n",
    "b2 = [1, 1]\n",
    "\n",
    "intersection_bin = get_intersect(a1, a2, b1, b2)[0]\n",
    "\n",
    "print (f'\\nIntersection: {intersection_bin:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reliability_results(observed, probability):\n",
    "    # Get observed class and predicted probability\n",
    "#    observed = y_data_bin\n",
    "#    probability = y_probs_bin\n",
    "\n",
    "    # Bin data with numpy digitize (this will assign a bin to each case)\n",
    "    step = 0.10\n",
    "    bins = np.arange(step, 1+step, step)\n",
    "    digitized = np.digitize(probability, bins)\n",
    "        \n",
    "    # Put results in DataFrame\n",
    "    reliability = pd.DataFrame()\n",
    "    reliability['bin'] = digitized\n",
    "    reliability['probability'] = probability\n",
    "    reliability['observed'] = observed\n",
    "    classification = 1 * (probability > 0.5 )\n",
    "    reliability['correct'] = observed == classification\n",
    "    reliability['count'] = 1\n",
    "\n",
    "    # Summarise data by bin in new dataframe\n",
    "    reliability_summary = pd.DataFrame()\n",
    "\n",
    "    # Add bins and k-fold to summary\n",
    "    reliability_summary['bin'] = bins\n",
    "\n",
    "    # Calculate mean of predicted probability of thrombolysis in each bin\n",
    "    reliability_summary['confidence'] = \\\n",
    "        reliability.groupby('bin').mean()['probability']\n",
    "\n",
    "    # Calculate the proportion of patients who receive thrombolysis\n",
    "    reliability_summary['fraction_positive'] = \\\n",
    "        reliability.groupby('bin').mean()['observed']\n",
    "\n",
    "    # Calculate proportion correct in each bin\n",
    "    reliability_summary['fraction_correct'] = \\\n",
    "        reliability.groupby('bin').mean()['correct']\n",
    "\n",
    "    # Calculate fraction of results in each bin\n",
    "    reliability_summary['fraction_results'] = \\\n",
    "        reliability.groupby('bin').sum()['count'] / reliability.shape[0]\n",
    "    \n",
    "    return(reliability_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_callibration(reliability_summary):\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    # Plot predicted prob vs fraction psotive\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "    # Loop through k-fold reliability results\n",
    "    x = reliability_summary['confidence']\n",
    "    y = reliability_summary['fraction_positive']\n",
    "    ax1.plot(x,y, color='orange')\n",
    "    # Add 1:1 line\n",
    "    ax1.plot([0,1],[0,1], color='k', linestyle ='--')\n",
    "    # Refine plot\n",
    "    ax1.set_xlabel('Model probability')\n",
    "    ax1.set_ylabel('Fraction positive')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "\n",
    "    ax1.grid()\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "    #plt.savefig(f'./output/{notebook}_{model_text}_reliability.jpg', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_summary = calculate_reliability_results(y_data_bin, y_probs_bin[:,1])\n",
    "plot_callibration(reliability_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Predict binary death (use threshold for sensitivity v. specificity intersection)\n",
    "\n",
    "#### Fit XGBoost model, train model with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the threshold that gave this intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min(enumerate(sensitivity), key=lambda x: abs(x[1]-intersection))\n",
    "threshold_intersection_bin = min(range(len(sensitivity)), \n",
    "                             key=lambda i: abs(sensitivity[i]-intersection_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix with this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_intersection_bin = y_probs_bin[:,1] > thresholds[threshold_intersection_bin]\n",
    "\n",
    "accuracy = calculate_accuracy(y_data_bin, y_pred_intersection_bin)\n",
    "print (f'Accuracy (binary model) [intersect threshold {thresholds[threshold_intersection_bin]:0.4f}]: {accuracy[\"accuracy\"]:0.2f}')\n",
    "print (f'Sensitivity (binary model) [intersect threshold {thresholds[threshold_intersection_bin]:0.4f}]: {accuracy[\"sensitivity\"]:0.2f}')\n",
    "print (f'Specificity (binary model) [intersect threshold {thresholds[threshold_intersection_bin]:0.4f}]: {accuracy[\"specificity\"]:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_data_bin, y_pred_intersection_bin)\n",
    "\n",
    "# Visualize the confusion matrix with colors\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "heatmap = plt.imshow(cm, cmap=plt.cm.Blues, origin='lower')\n",
    "colorbar = plt.colorbar(heatmap, shrink=0.8, ax=ax1, alpha=0.5, label='Count')\n",
    "\n",
    "# To add values to plot\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        plt.text(j, i, cm[i][j], ha='center', va='center', color='r')\n",
    "\n",
    "ax1.set_xlabel('Predicted value')\n",
    "ax1.set_ylabel('Observed value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Predict multiclass death (use default threshold, 0.5)\n",
    "\n",
    "#### Fit XGBoost model, train model with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{paths.notebook}{paths.model_text}_mc.p\"\n",
    "\n",
    "# Check if exists\n",
    "file_exists = exists(filename)\n",
    "\n",
    "if file_exists:\n",
    "    # load model\n",
    "    with open(filename, 'rb') as filehandler:\n",
    "        model_mc = pickle.load(filehandler)\n",
    "else:        \n",
    "\n",
    "    # Define model\n",
    "    model_mc = XGBClassifier(verbosity = 0, seed=42, learning_rate=0.5)\n",
    "\n",
    "    # Fit model\n",
    "    model_mc.fit(X_data, y_data_mc)\n",
    "\n",
    "    # Save model\n",
    "    with open(filename, 'wb') as filehandler:\n",
    "        pickle.dump(model_mc, filehandler)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs_mc = model_mc.predict_proba(X_data)\n",
    "y_pred_mc = model_mc.predict(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get just the probability for death (outcome 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mc6 = y_probs_mc[:,6] > 0.5\n",
    "\n",
    "#y_data_mc6 = (y_data_mc == 6) * 1\n",
    "\n",
    "y_error_mc6 = y_data_bin - y_pred_mc6\n",
    "\n",
    "accuracy = np.mean(y_error_mc6==0)\n",
    "print (f'Accuracy (multiclass) [default threshold]: {accuracy:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_data_bin, y_pred_mc6)\n",
    "\n",
    "# Visualize the confusion matrix with colors\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "heatmap = plt.imshow(cm, cmap=plt.cm.Blues, origin='lower')\n",
    "colorbar = plt.colorbar(heatmap, shrink=0.8, ax=ax1, alpha=0.5, label='Count')\n",
    "\n",
    "# To add values to plot\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        plt.text(j, i, cm[i][j], ha='center', va='center', color='r')\n",
    "\n",
    "ax1.set_xlabel('Predicted value')\n",
    "ax1.set_ylabel('Observed value')\n",
    "ax1.set_title('Binary model using default threshold (0.5)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two different models (binary vs multiclass) predict the same outcome for each individual patient (when use default threshold, 0.5)?\n",
    "\n",
    "Plot a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_pred_bin, y_pred_mc6)\n",
    "\n",
    "# Visualize the confusion matrix with colors\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "heatmap = plt.imshow(cm, cmap=plt.cm.Blues, origin='lower')\n",
    "colorbar = plt.colorbar(heatmap, shrink=0.8, ax=ax1, alpha=0.5, label='Count')\n",
    "\n",
    "# To add values to plot\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        plt.text(j, i, cm[i][j], ha='center', va='center', color='r')\n",
    "\n",
    "ax1.set_xlabel('Predicted value from multiclass model')\n",
    "ax1.set_ylabel('Predicted value from binary model')\n",
    "ax1.set_title('Comparing two models using default threshold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_data_bin, y_probs_mc[:,6])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Show area under curve  \n",
    "print (f'\\nAUC: {roc_auc:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = []\n",
    "specificity = []\n",
    "\n",
    "# Get classification probabilities for k-fold replicate\n",
    "observed = y_data_bin\n",
    "proba = y_probs_mc[:,6]\n",
    "\n",
    "# Loop through increments in probability of survival\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "for cutoff in thresholds: #  loop 0 --> 1 on steps of 0.1\n",
    "    # Get classificiation using cutoff\n",
    "    predicted_class = proba >= cutoff\n",
    "    predicted_class = predicted_class * 1.0\n",
    "    # Call accuracy measures function\n",
    "    accuracy = calculate_accuracy(observed, predicted_class)\n",
    "    # Add accuracy scores to lists\n",
    "    sensitivity.append(accuracy['sensitivity'])\n",
    "    specificity.append(accuracy['specificity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# Plot ROC\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(fpr, tpr, color='orange')\n",
    "ax1.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Receiver Operator Characteristic Curve')\n",
    "text = f'AUC: {roc_auc:.3f}'\n",
    "ax1.text(0.64,0.07, text, \n",
    "         bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot sensitivity-specificity\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(sensitivity, specificity)\n",
    "ax2.set_xlabel('Sensitivity')\n",
    "ax2.set_ylabel('Specificity')\n",
    "ax2.set_title('Sensitivity-Specificity Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "filename = f\"{paths.notebook}{paths.model_text}_mc_roc_sens_spec.jpg\"\n",
    "plt.savefig(filename, dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['sensitivity'] = np.array(sensitivity)\n",
    "df['specificity'] = np.array(specificity)\n",
    "df['spec greater sens'] = df['specificity'] > df['sensitivity']\n",
    "\n",
    "# find last index for senitivity being greater than specificity \n",
    "mask = df['spec greater sens'] == False\n",
    "last_id_sens_greater_spec = np.max(df[mask].index)\n",
    "locs = [last_id_sens_greater_spec, last_id_sens_greater_spec + 1]\n",
    "points = df.iloc[locs][['sensitivity', 'specificity']]\n",
    "\n",
    "# Get intersetction with line of x=y\n",
    "a1 = list(points.iloc[0].values)\n",
    "a2 = list(points.iloc[1].values)\n",
    "b1 = [0, 0]\n",
    "b2 = [1, 1]\n",
    "\n",
    "intersection_mc6 = get_intersect(a1, a2, b1, b2)[0]\n",
    "\n",
    "print (f'\\nIntersection: {intersection_mc6:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_summary_mc6 = calculate_reliability_results(y_data_bin, y_probs_mc[:,6])\n",
    "plot_callibration(reliability_summary_mc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Predict multiclass death (use threshold for sensitivity v. specificity intersection)\n",
    "\n",
    "#### Fit XGBoost model, train model with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find threshold to give this intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_intersection_mc6 = min(range(len(sensitivity)), \n",
    "                             key=lambda i: abs(sensitivity[i]-intersection_mc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_intersection_mc6 = y_probs_mc[:,6] > thresholds[threshold_intersection_mc6]\n",
    "\n",
    "accuracy = calculate_accuracy(y_data_bin, y_pred_intersection_mc6)\n",
    "print (f'Accuracy (binary model) [intersect threshold {thresholds[threshold_intersection_mc6]:0.4f}]: {accuracy[\"accuracy\"]:0.2f}')\n",
    "print (f'Sensitivity (binary model) [intersect threshold {thresholds[threshold_intersection_mc6]:0.4f}]: {accuracy[\"sensitivity\"]:0.2f}')\n",
    "print (f'Specificity (binary model) [intersect threshold {thresholds[threshold_intersection_mc6]:0.4f}]: {accuracy[\"specificity\"]:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix with this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_data_bin, y_pred_intersection_mc6)\n",
    "\n",
    "# Visualize the confusion matrix with colors\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "heatmap = plt.imshow(cm, cmap=plt.cm.Blues, origin='lower')\n",
    "colorbar = plt.colorbar(heatmap, shrink=0.8, ax=ax1, alpha=0.5, label='Count')\n",
    "\n",
    "# To add values to plot\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        plt.text(j, i, cm[i][j], ha='center', va='center', color='r')\n",
    "\n",
    "ax1.set_xlabel('Predicted value')\n",
    "ax1.set_ylabel('Observed value')\n",
    "ax1.set_title('Binary model using intersection threshold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two different models (binary vs multiclass) predict the same outcome for each individual patient (when use the threshold to give intersect for sensitivity and specificity)?\n",
    "\n",
    "Plot a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_pred_intersection_bin, y_pred_intersection_mc6)\n",
    "\n",
    "# Visualize the confusion matrix with colors\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "heatmap = plt.imshow(cm, cmap=plt.cm.Blues, origin='lower')\n",
    "colorbar = plt.colorbar(heatmap, shrink=0.8, ax=ax1, alpha=0.5, label='Count')\n",
    "\n",
    "# To add values to plot\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        plt.text(j, i, cm[i][j], ha='center', va='center', color='r')\n",
    "\n",
    "ax1.set_xlabel('Predicted value from multiclass model')\n",
    "ax1.set_ylabel('Predicted value from binary model')\n",
    "ax1.set_title('Comparing two models using intersection threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('sam10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f85b883bff9a8a9f39576b94acbdf6672b3dc17c35647e7395f81e785740a4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
