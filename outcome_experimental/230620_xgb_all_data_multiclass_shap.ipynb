{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "### Plain English summary\n",
    "Machine learning algorithms (such as XGBoost) were devised to deal with enormous and complex datasets, with the approach that the more data that you can throw at them, the better, and let the algorithms work it out themselves.\n",
    "\n",
    "However this approach can make it tricky to be able to explain a coherent story about how the models are working, the relationships that they have found, and how they have made their predictions.\n",
    "\n",
    "Our machine learning work has taken on an additional focus - to make our work as explainable as possible. Both in terms of being able to explain how the models have arrived at their outcome, and in the ease at which we can disseminate our work to a wider audience. For us to have explainable models we want to have a balance between model complexity and model accuracy in order to be able to explain our models, whilst maintaining model performance.\n",
    "\n",
    "In this notebook we create a model to predict if a patient should receive thrombolysis using just a single input feature, chosen as the feature that gave the model it's best performance. The single feature that gave the best model performance was \"Arrival-to-scan time\". Fixing this feature in the model, we repeated the process to chose the next single feature to add to the model. The best single feature to include next was \"Stroke type\". We repeated this process, choosing the next feature to add to the model until 25 features were included (it was limited to 25 features for computational time purposes).\n",
    "\n",
    "We found that a model with eight features is able to provide 99% of the accuracy obtained when all 84 features are used, and that these eight features are also independent of each other (refer to section Check correlation between selected features to confirm this).\n",
    "\n",
    "When disseminating the initial 8 feature model outputs to clinicians we observed how, when they were discussing whether a particular patient was suitable to recieve thrombolysis, they would often discuss the patients age. Patient age was the 10th feature to be selected by this process. We decided to extend the feature selected list to include the 9th and 10th selected features: onset during sleep and patient age. This model provided >99% of the accuracy obtained when all 84 features are used. These ten features are also largely independent of each other (refer to section Check correlation between selected features to confirm this).\n",
    "\n",
    "This is not saying that these are the 10 most important features, as another highly correlated feature may also have been important, but it is now not needed to be included in the model.\n",
    "\n",
    "We will train future models using these ten features.\n",
    "\n",
    "NOTE: This experiment was performed using data where time from onset to arrival, and tiem from arrival to scan, were rounded to the nearest 5 minutes. When more precise data is used feature order varies slightly after feature 8.\n",
    "\n",
    "### Model and data\n",
    "XGBoost models were trained on stratified k-fold cross-validation data. The full dataset contains 84 features that describe the patient (in terms of their clinical characteristics, the stroke pathway, and the stroke team that they attended). Features to be included in the model were sequentially selected as the single best feature to add to the model in terms of performance from the area under the receiver operating characteristic (ROC AUC) curve. When included, the hospital feature is included as a one-hot encoded feature.\n",
    "\n",
    "### Aims\n",
    "Select up to 25 features (from the full set of 84 features) using forward feature selection. Features are selected sequentially (using the greedy approach), choosing the feature that leads to most improvement in ROC AUC score.\n",
    "Decide on the number of features to include in future models\n",
    "\n",
    "### Observations\n",
    "Ten features are able to provide a ROC AUC of 0.919 out of a maximum of 0.922. These features are also largely independent of each other.\n",
    "\n",
    "Our best model with 1, 2, 10 & 84 features had a ROC AUC of 0.715, 0.792, 0.919 & 0.922."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import math\n",
    "\n",
    "import importlib\n",
    "# Import local package\n",
    "from utils import waterfall\n",
    "# Force package to be reloaded\n",
    "importlib.reload(waterfall);\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the time duration to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    '''Singleton object for storing paths to data and database.'''\n",
    "\n",
    "  #  data_path: str = '../'\n",
    "  #  data_filename: str = 'SAMueL ssnap extract v2.csv'\n",
    "  #  data_save_path: str = './'\n",
    "  #  data_save_filename: str = 'reformatted_data.csv'\n",
    "  #  database_filename: str = 'samuel.db'\n",
    "  #  notebook: str = '01'\n",
    "  #  kfold_folder: str = 'data/kfold_5fold/'\n",
    "\n",
    "    data_read_path: str = '../data/'\n",
    "    data_read_filename: str = '02_reformatted_data_ml_230612.csv'\n",
    " #   data_save_path: str = './kfold_5fold'\n",
    "#    data_save_filename: str = 'train.csv'\n",
    "    notebook: str = '230620_'\n",
    "    model_text: str = 'xgb_all_data_5_features'\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Data has previously been split into 5 stratified k-fold splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths.data_read_path + paths.data_read_filename\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = data['discharge_disability'].unique()\n",
    "class_names = np.sort(class_names)\n",
    "n_classes = len(class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp_set = set(train_data[0].columns)\n",
    "\n",
    "if 'weekday' in temp_set:\n",
    "    print (\"weekday\")\n",
    "if 'discharge_destination' in temp_set:\n",
    "    print (\"distination\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for feature, prefix in zip(features_to_one_hot, list_prefix):\n",
    "    train_data[0] = convert_feature_to_one_hot(train_data[0], feature, prefix)\n",
    "\n",
    "temp_set = set(train_data[0].columns)\n",
    "if 'weekday' in temp_set:\n",
    "    print (\"weekday\")\n",
    "if 'discharge_destination' in temp_set:\n",
    "    print (\"dich\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_feature = pd.get_dummies(\n",
    "    df[feature_name], prefix = prefix)\n",
    "df = pd.concat([df, df_feature], axis=1)\n",
    "df.drop(feature_name, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(data)\n",
    "print(f\"There are {len(features)} features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to use onset to thrombolysis time in the model. Define function to calculate the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_onset_to_thrombolysis(row):\n",
    "    # Set default value of onset to thrombolysis of -100 (no thrombolysis given)\n",
    "    onset_to_thrombolysis = -100\n",
    "    # Set value if thrombolysis given\n",
    "    if  row['scan_to_thrombolysis_time'] != -100:\n",
    "        onset_to_thrombolysis = (row['onset_to_arrival_time'] + \n",
    "        row['arrival_to_scan_time'] + row['scan_to_thrombolysis_time'])\n",
    "    return onset_to_thrombolysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate onset to thgrombolysis (but set to -100 if no thrombolysis given)\n",
    "data['onset_to_thrombolysis_time'] = data.apply(calculate_onset_to_thrombolysis, axis=1)\n",
    "data.drop(['scan_to_thrombolysis_time', 'arrival_to_scan_time',\n",
    "        'onset_to_arrival_time'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only include the elected 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['prior_disability','stroke_severity','stroke_team',\n",
    "                     'onset_to_thrombolysis_time','age']\n",
    "selected_features.append('discharge_disability')\n",
    "data = data[selected_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot the categorical features\n",
    "\n",
    "Convert some categorical features to one hot encoded features.\n",
    "\n",
    "Define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_to_one_hot(df, feature_name, prefix):\n",
    "    \"\"\"\n",
    "    df [dataframe]: training or test dataset\n",
    "    feature_name [str]: feature to convert to ont hot encoding\n",
    "    prefix [str]: string to use on new feature\n",
    "    \"\"\"\n",
    "\n",
    "    # One hot encode a feature\n",
    "    df_feature = pd.get_dummies(\n",
    "        df[feature_name], prefix = prefix)\n",
    "    df = pd.concat([df, df_feature], axis=1)\n",
    "    df.drop(feature_name, axis=1, inplace=True)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up two lists for the one hot encoding. \n",
    "\n",
    "A list of the feature names that are categorical and to be converted using one hot encoding.\n",
    "A list of the prefixes to use for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_one_hot = [\"stroke_team\"]\n",
    "list_prefix = [\"team\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature in the list, for each train and test dataset, convert to one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, prefix in zip(features_to_one_hot, list_prefix):\n",
    "    data = convert_feature_to_one_hot(data, feature, prefix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data.drop('discharge_disability', axis=1)\n",
    "y_data = data['discharge_disability']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of features in dataset, post one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ohe = list(X_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit XGBoost model\n",
    "\n",
    "Train model with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{paths.notebook}{paths.model_text}.p\"\n",
    "\n",
    "# Check if exists\n",
    "file_exists = exists(filename)\n",
    "\n",
    "if file_exists:\n",
    "    # load model\n",
    "    with open(filename, 'rb') as filehandler:\n",
    "        model = pickle.load(filehandler)\n",
    "else:        \n",
    "\n",
    "    # Define model\n",
    "    model = XGBClassifier(verbosity = 0, seed=42, learning_rate=0.5)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_data, y_data)\n",
    "\n",
    "    # Save model\n",
    "    with open(filename, 'wb') as filehandler:\n",
    "        pickle.dump(model, filehandler)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = model.predict_proba(X_data)\n",
    "y_pred = model.predict(X_data)\n",
    "\n",
    "# Calculate error\n",
    "y_error = y_data - y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show accuracy (identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(y_error==0)\n",
    "print (f'Accuracy: {accuracy:0.2f}')\n",
    "\n",
    "error_within_one = np.mean(np.abs(y_error)<=1)\n",
    "print (f'Error within 1: {error_within_one:0.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and store feature importances\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Store in DataFrame\n",
    "feature_importance_df = pd.DataFrame(data = feature_importance, index=features_ohe)\n",
    "feature_importance_df.columns = ['importance']\n",
    "\n",
    "# Sort by importance (weight)\n",
    "feature_importance_df.sort_values(by='importance', \n",
    "                                  ascending=False, inplace=True)\n",
    "\n",
    "# Save\n",
    "#feature_importance_df.to_csv(f'output/{notebook}_{model_type}_feature_importance.csv')\n",
    "\n",
    "# Display top 25\n",
    "feature_importance_df.head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bar chart for the XGBoost feature importance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Get labels and values\n",
    "labels = feature_importance_df.index.values[0:25]\n",
    "pos = np.arange(len(labels))\n",
    "val = feature_importance_df['importance'].values[0:25]\n",
    "\n",
    "# Plot\n",
    "ax.bar(pos, val)\n",
    "ax.set_ylabel('Feature importance')\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f'output/{notebook}_{model_type}_feature_weights_bar.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same data in another display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = 20\n",
    "indices = np.argsort(feature_importance)\n",
    "indices = indices[-n_show:]\n",
    "features = X_data.columns\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), feature_importance[indices], color='g', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP values\n",
    "SHAP values give the contribution that each feature has on the models prediction, per instance. A SHAP value is returned for each feature, for each instance.\n",
    "\n",
    "We will use the shap library: https://shap.readthedocs.io/en/latest/index.html\n",
    "\n",
    "'Raw' SHAP values from XGBoost model are log odds ratios. A SHAP value is returned for each feature, for each instance, for each model (one per k-fold)\n",
    "\n",
    "## Get SHAP values\n",
    "TreeExplainer is a fast and exact method to estimate SHAP values for tree models and ensembles of trees. Using this we can calculate the SHAP values.\n",
    "\n",
    "Either load from pickle (if file exists), or calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = (f'{paths.notebook}{paths.model_text}_shap_values_extended.p')\n",
    "# Check if exists\n",
    "file_exists = exists(filename)\n",
    "\n",
    "if file_exists:\n",
    "\n",
    "    # Load shap values\n",
    "    with open(filename, 'rb') as filehandler:\n",
    "        shap_values_extended = pickle.load(filehandler)\n",
    "        shap_values = shap_values_extended.values\n",
    "\n",
    "    # Load explainer\n",
    "    explainer_filename = (f'{paths.notebook}{paths.model_text}_shap_explainer.p')\n",
    "    with open(explainer_filename, 'rb') as filehandler:\n",
    "        explainer = pickle.load(filehandler)\n",
    "else:\n",
    "\n",
    "\n",
    "    # Set up explainer using the model and feature values from training set\n",
    "    explainer = shap.TreeExplainer(model, X_data)\n",
    "\n",
    "    # Get (and store) Shapley values along with base and feature values\n",
    "    shap_values_extended = explainer(X_data)\n",
    "\n",
    "    # Shap values exist for each classification in a Tree\n",
    "    # We are interested in 1=give thrombolysis (not 0=not give thrombolysis)\n",
    "    shap_values = shap_values_extended.values\n",
    "\n",
    "    explainer_filename = (f'{paths.notebook}{paths.model_text}_shap_explainer.p')\n",
    "\n",
    "    # Save explainer using pickle\n",
    "    with open(explainer_filename, 'wb') as filehandler:\n",
    "        pickle.dump(explainer, filehandler)\n",
    "        \n",
    "    # Save shap values extendedr using pickle\n",
    "    with open(filename, 'wb') as filehandler:\n",
    "        pickle.dump(shap_values_extended, filehandler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the 3D numpy array (shap_values) match the format required by shap.summary_plot - a list of 7 arrays (shap_values_list).\n",
    "\n",
    "Only include the number of features want for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_display = 0\n",
    "end_display = 4\n",
    "\n",
    "shap_values_list = []\n",
    "for i in range(n_classes):\n",
    "    shap_values_list.append(shap_values[:,st_display:end_display,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "#fig.legend(loc=4)\n",
    "ax = shap.summary_plot(shap_values_list, X_data.iloc[:,st_display:end_display].values, \n",
    "                       plot_type=\"bar\", \n",
    "                       class_names=model.classes_, \n",
    "                       feature_names = X_data.iloc[:,st_display:end_display].columns, \n",
    "                       class_inds=\"original\",\n",
    "                       show=False)\n",
    "#fig.legend(loc=4)\n",
    "#ax.legend(loc=4)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_display = 4\n",
    "end_display = 50\n",
    "\n",
    "shap_values_list = []\n",
    "for i in range(n_classes):\n",
    "    shap_values_list.append(shap_values[:,st_display:end_display,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "#fig.legend(loc=4)\n",
    "ax = shap.summary_plot(shap_values_list, X_data.iloc[:,st_display:end_display].values, \n",
    "                       plot_type=\"bar\", \n",
    "                       class_names=model.classes_, \n",
    "                       feature_names = X_data.iloc[:,st_display:end_display].columns, \n",
    "                       class_inds=\"original\",\n",
    "                       show=False)\n",
    "#fig.legend(loc=4)\n",
    "#ax.legend(loc=4)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can look at an individual class. Here each subplot shows the results for a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_display = 0\n",
    "end_display = 4\n",
    "\n",
    "shap_values_list = []\n",
    "for i in range(n_classes):\n",
    "    shap_values_list.append(shap_values[:,st_display:end_display,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,50))\n",
    "for i in range(n_classes):  \n",
    "    ax = fig.add_subplot(1,n_classes,i+1)\n",
    "    ax = shap.summary_plot(shap_values_list[i], \n",
    "                           X_data.iloc[:,st_display:end_display].values, \n",
    "                           feature_names=X_data.iloc[:,st_display:end_display].columns, \n",
    "                           show=False, auto_size_plot=False)\n",
    "    #ax.title(f\"Class {model.classes_[i]}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3,figsize=(10,10))\n",
    "row=1\n",
    "col=1\n",
    "for i in range(n_classes):  \n",
    "    shap.summary_plot(shap_values_list[i], X_data.iloc[:,st_display:end_display].values, ax=axes[row,col],\n",
    "                      feature_names=X_data.iloc[:,st_display:end_display].columns, show=False)\n",
    "    col+=1\n",
    "    if col==4:\n",
    "        col=1\n",
    "        row+=1\n",
    "    #ax.title(f\"Class {model.classes_[i]}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3,figsize=(15,15))\n",
    "row=0\n",
    "col=0\n",
    "for i in range(n_classes):  #(1):\n",
    "    shap.summary_plot(shap_values_list[i], X_data.iloc[:,st_display:end_display].values, \n",
    "                      feature_names=X_data.iloc[:,st_display:end_display].columns, show=False)#matplotlib=True)\n",
    "    f = plt.gcf()\n",
    "    #ax = axes[0,0]#row,col]\n",
    "    #ax = f\n",
    "   # col+=1\n",
    "    #if col==3:\n",
    "   #     col=0\n",
    "   #     row+=1\n",
    "    #ax.title(f\"Class {model.classes_[i]}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3,\n",
    "                         ncols=3)\n",
    "axes = axes.ravel()\n",
    "\n",
    "fig.suptitle(f'Summary plot for all classes', fontsize=15)\n",
    "\n",
    "count = 0\n",
    "for i in range(1):#n_classes):  \n",
    "        \n",
    "    shap.summary_plot(shap_values_list[i], X_data.values, \n",
    "                      feature_names=X_data.columns, show=False,\n",
    "                      ax=axes[count])\n",
    "    count+=1\n",
    "\n",
    "# Visual propoerties of figure\n",
    "dimension = 5 * 5\n",
    "fig.set_figheight(dimension)\n",
    "fig.set_figwidth(dimension)\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP dependency plot\n",
    "\n",
    "The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 2001 [3]). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. The partial dependence plot is a global method: The method considers all instances and gives a statement about the global relationship of a feature with the predicted outcome. An assumption of the PDP is that the first feature are not correlated with the second feature. If this assumption is violated, the averages calculated for the partial dependence plot will include data points that are very unlikely or even impossible. A dependence plot is a scatter plot that shows the effect a single feature has on the predictions made by the model. In this example the property value increases significantly when the average number of rooms per dwelling is higher than 6. Each dot is a single prediction (row) from the dataset. The x-axis is the actual value from the dataset. The y-axis is the SHAP value for that feature, which represents how much knowing that feature’s value changes the output of the model for that sample’s prediction. The color corresponds to a second feature that may have an interaction effect with the feature we are plotting (by default this second feature is chosen automatically). If an interaction effect is present between this other feature and the feature we are plotting it will show up as a distinct vertical pattern of coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_list = []\n",
    "for i in range(n_classes):\n",
    "    shap_values_list.append(shap_values[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we pass a numpy array instead of a data frame then we\n",
    "# need pass the feature names in separately\n",
    "shap.dependence_plot(ind=features_ohe[0], \n",
    "                     interaction_index=features_ohe[1], \n",
    "                     shap_values=shap_values_list[0], features=X_data.values, \n",
    "                     feature_names=X_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_display = 4\n",
    "\n",
    "# Create a matrix of subplots per class. Each showing the relationship between\n",
    "# each combination of features on the SHAP value.\n",
    "for c in range(n_classes):\n",
    "    # setup matrix of subplots\n",
    "    fig, axes = plt.subplots(nrows=max_display,\n",
    "                             ncols=max_display)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Set overall title\n",
    "    fig.suptitle(f'Class {model.classes_[c]}', fontsize=30)\n",
    "\n",
    "    # Initialise subplot counter\n",
    "    count = 0\n",
    "\n",
    "    # Loop through the features to display\n",
    "    for i in range(max_display):\n",
    "        # Loop through the features to display\n",
    "        for j in range(max_display):\n",
    "            # Create the plot. Pass the axes\n",
    "            shap.dependence_plot(ind=features_ohe[i], \n",
    "                                interaction_index=features_ohe[j], \n",
    "                                shap_values=shap_values_list[c], \n",
    "                                features=X_data.values, \n",
    "                                feature_names=X_data.columns,\n",
    "                                show=False, ax=axes[count])\n",
    "            \n",
    "            # Add line as shap=0\n",
    "            axes[count].plot([-1, X_data[features_ohe[i]].max()+1],\n",
    "                             [0,0],c='0.5')\n",
    "            \n",
    "            # Increase subplot counter\n",
    "            count+=1\n",
    "    \n",
    "    # Change font size for each subplot\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=15)\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=15)\n",
    "        ax.tick_params(axis='both',which='major',labelsize=15)\n",
    "\n",
    "    # Visual propoerties of figure\n",
    "    dimension = 5 * 5\n",
    "    fig.set_figheight(dimension)\n",
    "    fig.set_figwidth(dimension)\n",
    "    fig.tight_layout(pad=2)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Force plot\n",
    "\n",
    "Force plot gives us the explainability of a single model prediction. In this plot we can see how features contributed to the model’s prediction for a specific observation. It is very convenient to use for error analysis or for a deep understanding of a particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0], shap_values_list[0][row], \n",
    "                X_data.values[row], feature_names = X_data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP waterfall plot\n",
    "\n",
    "Waterfall is another local analysis plot of a single instance prediction. Let’s take instance number 8 as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class = 0\n",
    "shap.waterfall_plot(shap.Explanation(values=shap_values_list[plot_class][row], \n",
    "                                        base_values=explainer.expected_value[plot_class], data=X_data.iloc[row],  \n",
    "                                        feature_names=X_data.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class = 1\n",
    "shap.waterfall_plot(shap.Explanation(values=shap_values_list[plot_class][row], \n",
    "                                        base_values=explainer.expected_value[plot_class], data=X_data.iloc[row],  \n",
    "                                        feature_names=X_data.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of subplots per class. Each showing the relationship between\n",
    "# each combination of features on the SHAP value.\n",
    "# setup matrix of subplots\n",
    "fig, axes = plt.subplots(nrows=3,\n",
    "                         ncols=3)\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Set overall title\n",
    "fig.suptitle(f'Waterfall for instance {row}', fontsize=30)\n",
    "\n",
    "# Initialise subplot counter\n",
    "count = 0\n",
    "\n",
    "for c in range(n_classes):\n",
    "    shap.waterfall_plot(shap.Explanation(values=shap_values_list[plot_class][row], \n",
    "                                        base_values=explainer.expected_value[plot_class], data=X_data.iloc[row],  \n",
    "                                        feature_names=X_data.columns.tolist()), show=False, ax=axes[count])\n",
    "    \n",
    "    # Increase subplot counter\n",
    "    count+=1\n",
    "    \n",
    "# Change font size for each subplot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=15)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=15)\n",
    "    ax.tick_params(axis='both',which='major',labelsize=15)\n",
    "\n",
    "# Visual propoerties of figure\n",
    "dimension = 5 * 5\n",
    "fig.set_figheight(dimension)\n",
    "fig.set_figwidth(dimension)\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of subplots per class. Each showing the relationship between\n",
    "# each combination of features on the SHAP value.\n",
    "# setup matrix of subplots\n",
    "fig = plt.figure()\n",
    "\n",
    "# Set overall title\n",
    "fig.suptitle(f'Waterfall for instance {row} '\n",
    "             f'(observed dischange mRS = {y_data.iloc[row]})', \n",
    "             fontsize=30)\n",
    "\n",
    "# Initialise subplot counter\n",
    "count = 1\n",
    "\n",
    "for c in range(n_classes):\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    shap.waterfall_plot(shap.Explanation(values=shap_values_list[c][row], \n",
    "                                        base_values=explainer.expected_value[c], \n",
    "                                        data=X_data.iloc[row],  \n",
    "                                        feature_names=X_data.columns.tolist()), \n",
    "                                        show=False)\n",
    "    ax. set_title(f\"Predicted discharge mRS {c}\")\n",
    "    # Increase subplot counter\n",
    "    count+=1\n",
    "    \n",
    "plt.gcf().set_size_inches(20,15)\n",
    "plt.tight_layout()\n",
    "# Change font size for each subplot\n",
    "#for ax in axes:\n",
    "#    ax.set_xlabel(ax.get_xlabel(), fontsize=15)\n",
    "#    ax.set_ylabel(ax.get_ylabel(), fontsize=15)\n",
    "#    ax.tick_params(axis='both',which='major',labelsize=15)\n",
    "\n",
    "# Visual propoerties of figure\n",
    "#dimension = 5 * 5\n",
    "#fig.set_figheight(dimension)\n",
    "#fig.set_figwidth(dimension)\n",
    "#fig.tight_layout(pad=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_waterfall_multiclass_grid(row, y_data_row, n_classes, shap_values_list_row,\n",
    "                                     base_values, data, feature_names):\n",
    "\n",
    "    # Create a matrix of subplots per class. Each showing the relationship between\n",
    "    # each combination of features on the SHAP value.\n",
    "    # setup matrix of subplots\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Set overall title\n",
    "    fig.suptitle(f'Waterfall for instance {row} '\n",
    "                f'(observed dischange mRS = {y_data_row})', \n",
    "                fontsize=30)\n",
    "\n",
    "    # Initialise subplot counter\n",
    "    count = 1\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        ax = fig.add_subplot(4,2,count)\n",
    "        shap.waterfall_plot(shap.Explanation(values=shap_values_list_row[c], \n",
    "                                            base_values=base_values[c], \n",
    "                                            data=data,  \n",
    "                                            feature_names=feature_names), \n",
    "                                            show=False)\n",
    "        ax. set_title(f\"Predicted discharge mRS {c}\")\n",
    "        # Increase subplot counter\n",
    "        count+=1\n",
    "        \n",
    "    plt.gcf().set_size_inches(20,15)\n",
    "    plt.tight_layout()\n",
    "    # Change font size for each subplot\n",
    "    #for ax in axes:\n",
    "    #    ax.set_xlabel(ax.get_xlabel(), fontsize=15)\n",
    "    #    ax.set_ylabel(ax.get_ylabel(), fontsize=15)\n",
    "    #    ax.tick_params(axis='both',which='major',labelsize=15)\n",
    "\n",
    "    # Visual propoerties of figure\n",
    "    #dimension = 5 * 5\n",
    "    #fig.set_figheight(dimension)\n",
    "    #fig.set_figwidth(dimension)\n",
    "    #fig.tight_layout(pad=2)\n",
    "    plt.show()\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 1\n",
    "create_waterfall_multiclass_grid(row, y_data.iloc[row], n_classes, \n",
    "                                 shap_values_list[:][row],\n",
    "                                 explainer.expected_value, X_data.iloc[row], \n",
    "                                 X_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 0\n",
    "create_waterfall_multiclass_grid(row, y_data.iloc[row], n_classes, \n",
    "                                 shap_values_list[:][row],\n",
    "                                 explainer.expected_value, X_data.iloc[row], \n",
    "                                 X_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall.waterfall(shap_values_extended[plot_class][row], \n",
    "                    show=False, y_reverse=True, rank_absolute=False, \n",
    "                    raw_ascending=False)\n",
    "#                    base_values=explainer.expected_value[plot_class], \n",
    "#                                        data=X_data.iloc[row],  \n",
    "#                                        feature_names=X_data.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_extended[row].base_values[plot_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_extended.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_extended[row].values[:,plot_class].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_extended[row]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_extended[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('sam10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f85b883bff9a8a9f39576b94acbdf6672b3dc17c35647e7395f81e785740a4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
